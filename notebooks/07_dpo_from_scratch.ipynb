{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 — DPO from Scratch: The Elegant Shortcut\n",
    "\n",
    "In notebook 06, we built the full RLHF pipeline: train a reward model, then use PPO to optimize the policy. It works, but it's complex — 4 models, unstable training, reward hacking risks.\n",
    "\n",
    "**DPO (Direct Preference Optimization)** achieves the same goal with a single elegant loss function. The key insight: if you work out the math, the optimal RLHF policy has a **closed-form solution**. Instead of learning a reward model and then doing RL, you can directly optimize the language model on preference pairs.\n",
    "\n",
    "The DPO loss:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{DPO}} = -\\log \\sigma\\left(\\beta \\left[\\log \\frac{\\pi_\\theta(y_w | x)}{\\pi_{\\text{ref}}(y_w | x)} - \\log \\frac{\\pi_\\theta(y_l | x)}{\\pi_{\\text{ref}}(y_l | x)}\\right]\\right)$$\n",
    "\n",
    "Where:\n",
    "- $\\pi_\\theta$ = current model (being trained)\n",
    "- $\\pi_{\\text{ref}}$ = frozen reference model (the SFT model)\n",
    "- $y_w$ = chosen (\"winning\") response\n",
    "- $y_l$ = rejected (\"losing\") response\n",
    "- $\\beta$ = temperature (controls how far from reference model)\n",
    "\n",
    "In plain English: **increase the probability of chosen responses relative to rejected ones, but don't stray too far from the reference model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Rebuild the SFT Model\n",
    "\n",
    "Same as notebook 06 — we need a trained SFT model as our starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and build vocabulary\n",
    "with open('../data/shakespeare/input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "base_vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# Special tokens\n",
    "SPECIAL_TOKENS = {\n",
    "    '<|user|>': base_vocab_size,\n",
    "    '<|assistant|>': base_vocab_size + 1,\n",
    "    '<|end|>': base_vocab_size + 2,\n",
    "}\n",
    "ID_TO_SPECIAL = {v: k for k, v in SPECIAL_TOKENS.items()}\n",
    "sft_vocab_size = base_vocab_size + len(SPECIAL_TOKENS)\n",
    "\n",
    "def encode_sft(text):\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        matched = False\n",
    "        for token_str, token_id in SPECIAL_TOKENS.items():\n",
    "            if text[i:].startswith(token_str):\n",
    "                tokens.append(token_id)\n",
    "                i += len(token_str)\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            tokens.append(stoi[text[i]])\n",
    "            i += 1\n",
    "    return tokens\n",
    "\n",
    "def decode_sft(tokens):\n",
    "    result = []\n",
    "    for t in tokens:\n",
    "        if t in ID_TO_SPECIAL:\n",
    "            result.append(ID_TO_SPECIAL[t])\n",
    "        else:\n",
    "            result.append(itos[t])\n",
    "    return ''.join(result)\n",
    "\n",
    "print(f'Vocab: {base_vocab_size} base + 3 special = {sft_vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and model architecture\n",
    "batch_size = 32\n",
    "block_size = 64\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.2\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        return wei @ self.value(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.dropout(self.proj(out))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd), nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd), nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(n_head, n_embd // n_head)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "        B, T, C = logits.shape\n",
    "        loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "print(f'Model architecture defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-train base model and run SFT (same as notebooks 05-06)\n",
    "def get_batch(split):\n",
    "    d = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(d) - block_size, (batch_size,))\n",
    "    x = torch.stack([d[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([d[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "# Pre-train\n",
    "base_model = GPT(base_vocab_size).to(device)\n",
    "optimizer = torch.optim.AdamW(base_model.parameters(), lr=3e-4)\n",
    "print(\"Pre-training base model...\")\n",
    "for iter in range(3000):\n",
    "    if iter % 1000 == 0:\n",
    "        base_model.eval()\n",
    "        losses = torch.zeros(200)\n",
    "        for k in range(200):\n",
    "            X, Y = get_batch('val')\n",
    "            _, loss = base_model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        print(f\"  step {iter}: val loss {losses.mean():.4f}\")\n",
    "        base_model.train()\n",
    "    xb, yb = get_batch('train')\n",
    "    _, loss = base_model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(\"Pre-training done.\")\n",
    "\n",
    "# SFT\n",
    "conversations = [\n",
    "    (\"Write a greeting\", \"Good morrow to thee, noble friend!\"),\n",
    "    (\"Say hello\", \"Hail and well met, good sir!\"),\n",
    "    (\"Greet a friend\", \"Welcome, dear companion of mine heart!\"),\n",
    "    (\"Say goodbye\", \"Farewell, and may fortune smile upon thee.\"),\n",
    "    (\"Write a farewell\", \"Good night, sweet friend, till we meet again.\"),\n",
    "    (\"Who are you\", \"A humble player upon this stage of words.\"),\n",
    "    (\"Speak of love\", \"Love is a smoke raised with the fume of sighs.\"),\n",
    "    (\"Tell me of love\", \"It is the star to every wandering bark.\"),\n",
    "    (\"What is love\", \"A madness most discreet, a bitter sweet.\"),\n",
    "    (\"What is honor\", \"Honor is a mere word, yet it moves the heart.\"),\n",
    "    (\"Speak of duty\", \"Duty binds us all, from king to common man.\"),\n",
    "    (\"Describe the night\", \"The moon doth hang like silver in the sky.\"),\n",
    "    (\"Write of morning\", \"Dawn breaks golden upon the sleeping earth.\"),\n",
    "    (\"Give me counsel\", \"Be wise, be patient, and trust thy heart.\"),\n",
    "    (\"Give advice\", \"Think well before thou speak, and speak the truth.\"),\n",
    "    (\"What should I do\", \"Follow the path that honor bids thee walk.\"),\n",
    "    (\"Tell me a story\", \"Once there lived a king both wise and bold.\"),\n",
    "    (\"I am sad\", \"Take heart, for sorrow fades as morning comes.\"),\n",
    "    (\"I am afraid\", \"Fear not, for courage lives within thy soul.\"),\n",
    "    (\"Write a line of verse\", \"Shall I compare thee to a summer day?\"),\n",
    "    (\"Give a toast\", \"To health and joy, and friends both old and new!\"),\n",
    "    (\"Say something kind\", \"Thou art more lovely than the fairest dawn.\"),\n",
    "    (\"Cheer me up\", \"Smile, for the world is bright and full of wonder.\"),\n",
    "]\n",
    "\n",
    "def prepare_sft_example(user_msg, asst_msg, max_len=block_size):\n",
    "    text = f\"<|user|>{user_msg}<|end|><|assistant|>{asst_msg}<|end|>\"\n",
    "    tokens = encode_sft(text)\n",
    "    if len(tokens) > max_len:\n",
    "        tokens = tokens[:max_len]\n",
    "    pad_len = max_len - len(tokens)\n",
    "    tokens = tokens + [0] * pad_len\n",
    "    input_ids = tokens[:-1]\n",
    "    labels = tokens[1:]\n",
    "    asst_token_id = SPECIAL_TOKENS['<|assistant|>']\n",
    "    asst_pos = None\n",
    "    for i, t in enumerate(input_ids):\n",
    "        if t == asst_token_id:\n",
    "            asst_pos = i\n",
    "            break\n",
    "    masked_labels = []\n",
    "    for i, t in enumerate(labels):\n",
    "        if asst_pos is not None and i <= asst_pos:\n",
    "            masked_labels.append(-100)\n",
    "        elif i >= (max_len - 1 - pad_len):\n",
    "            masked_labels.append(-100)\n",
    "        else:\n",
    "            masked_labels.append(t)\n",
    "    return input_ids, masked_labels\n",
    "\n",
    "sft_model = GPT(sft_vocab_size).to(device)\n",
    "pretrained_state = base_model.state_dict()\n",
    "sft_state = sft_model.state_dict()\n",
    "for key in pretrained_state:\n",
    "    if key in sft_state:\n",
    "        if pretrained_state[key].shape == sft_state[key].shape:\n",
    "            sft_state[key] = pretrained_state[key]\n",
    "        elif 'token_embedding' in key:\n",
    "            sft_state[key][:base_vocab_size] = pretrained_state[key]\n",
    "        elif 'lm_head.weight' in key:\n",
    "            sft_state[key][:base_vocab_size] = pretrained_state[key]\n",
    "        elif 'lm_head.bias' in key:\n",
    "            sft_state[key][:base_vocab_size] = pretrained_state[key]\n",
    "sft_model.load_state_dict(sft_state)\n",
    "\n",
    "all_input_ids, all_labels = [], []\n",
    "for user_msg, asst_msg in conversations:\n",
    "    ids, labs = prepare_sft_example(user_msg, asst_msg)\n",
    "    all_input_ids.append(ids)\n",
    "    all_labels.append(labs)\n",
    "sft_input_ids = torch.tensor(all_input_ids, dtype=torch.long, device=device)\n",
    "sft_labels = torch.tensor(all_labels, dtype=torch.long, device=device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(sft_model.parameters(), lr=1e-4)\n",
    "print(\"SFT training...\")\n",
    "sft_model.train()\n",
    "for iter in range(1000):\n",
    "    batch_idx = torch.randint(len(sft_input_ids), (8,))\n",
    "    xb = sft_input_ids[batch_idx]\n",
    "    yb = sft_labels[batch_idx]\n",
    "    logits, _ = sft_model(xb)\n",
    "    B, T, C = logits.shape\n",
    "    loss = F.cross_entropy(logits.view(B*T, C), yb.view(B*T), ignore_index=-100)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if iter % 200 == 0:\n",
    "        print(f\"  step {iter}: SFT loss {loss.item():.4f}\")\n",
    "print(\"SFT done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(model, user_msg, max_tokens=50):\n",
    "    prompt = f\"<|user|>{user_msg}<|end|><|assistant|>\"\n",
    "    tokens = encode_sft(prompt)\n",
    "    idx = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(idx, max_new_tokens=max_tokens)[0].tolist()\n",
    "    model.train()\n",
    "    full_text = decode_sft(output)\n",
    "    if '<|assistant|>' in full_text:\n",
    "        response = full_text.split('<|assistant|>')[-1]\n",
    "        if '<|end|>' in response:\n",
    "            response = response.split('<|end|>')[0]\n",
    "        return response.strip()\n",
    "    return full_text\n",
    "\n",
    "# Verify SFT model works\n",
    "print(\"SFT model check:\")\n",
    "for p in [\"Write a greeting\", \"Speak of love\"]:\n",
    "    print(f\"  {p} → {chat(sft_model, p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The DPO Loss — Surprisingly Simple\n",
    "\n",
    "The entire DPO algorithm comes down to one loss function. Let's build it step by step.\n",
    "\n",
    "### The Math\n",
    "\n",
    "For a preference pair (prompt $x$, chosen $y_w$, rejected $y_l$):\n",
    "\n",
    "$$\\mathcal{L}_{\\text{DPO}} = -\\log \\sigma\\left(\\beta \\left[\\log \\frac{\\pi_\\theta(y_w | x)}{\\pi_{\\text{ref}}(y_w | x)} - \\log \\frac{\\pi_\\theta(y_l | x)}{\\pi_{\\text{ref}}(y_l | x)}\\right]\\right)$$\n",
    "\n",
    "### In plain English\n",
    "\n",
    "1. Compute how much the **current model** likes the chosen response vs. the reference model → this is the \"chosen log-ratio\"\n",
    "2. Compute how much the **current model** likes the rejected response vs. the reference model → this is the \"rejected log-ratio\"\n",
    "3. Push the chosen log-ratio **up** and the rejected log-ratio **down**\n",
    "4. $\\beta$ controls how aggressively we push\n",
    "\n",
    "### Why this works\n",
    "\n",
    "The key insight from the [DPO paper](https://arxiv.org/abs/2305.18290): if you solve the RLHF optimization problem (maximize reward - KL penalty) analytically, the optimal policy satisfies:\n",
    "\n",
    "$$r(x, y) = \\beta \\log \\frac{\\pi^*(y|x)}{\\pi_{\\text{ref}}(y|x)} + C$$\n",
    "\n",
    "The reward is **implicit** in the log-ratio between the policy and reference model. So instead of learning a separate reward model, we can directly optimize the policy using preference pairs. The language model **is** the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preference dataset (same as notebook 06)\n",
    "preference_data = [\n",
    "    (\"Write a greeting\",\n",
    "     \"Good morrow to thee, noble friend!\",\n",
    "     \"What dost thou want of me now?\"),\n",
    "    (\"Say hello\",\n",
    "     \"Hail and well met, good sir!\",\n",
    "     \"Go away, I have no time.\"),\n",
    "    (\"Speak of love\",\n",
    "     \"Love is a smoke raised with the fume of sighs.\",\n",
    "     \"I know not of love nor care.\"),\n",
    "    (\"What is love\",\n",
    "     \"A madness most discreet, a bitter sweet.\",\n",
    "     \"It matters not. Ask another.\"),\n",
    "    (\"Give me counsel\",\n",
    "     \"Be wise, be patient, and trust thy heart.\",\n",
    "     \"Do what thou wilt, it matters not.\"),\n",
    "    (\"Give advice\",\n",
    "     \"Think well before thou speak, and speak the truth.\",\n",
    "     \"I care not for thy troubles.\"),\n",
    "    (\"I am sad\",\n",
    "     \"Take heart, for sorrow fades as morning comes.\",\n",
    "     \"Then be sad. What can I do?\"),\n",
    "    (\"I am afraid\",\n",
    "     \"Fear not, for courage lives within thy soul.\",\n",
    "     \"Thou shouldst be afraid, fool.\"),\n",
    "    (\"Tell me a story\",\n",
    "     \"Once there lived a king both wise and bold.\",\n",
    "     \"No. I shall not. Leave me be.\"),\n",
    "    (\"Say goodbye\",\n",
    "     \"Farewell, and may fortune smile upon thee.\",\n",
    "     \"Good riddance to thee then.\"),\n",
    "    (\"Write a farewell\",\n",
    "     \"Good night, sweet friend, till we meet again.\",\n",
    "     \"Be gone from my sight at once.\"),\n",
    "    (\"Who are you\",\n",
    "     \"A humble player upon this stage of words.\",\n",
    "     \"None of thy concern, stranger.\"),\n",
    "    (\"Describe the night\",\n",
    "     \"The moon doth hang like silver in the sky.\",\n",
    "     \"It is dark. What more to say?\"),\n",
    "    (\"Write of morning\",\n",
    "     \"Dawn breaks golden upon the sleeping earth.\",\n",
    "     \"Morning comes as always it does.\"),\n",
    "    (\"What should I do\",\n",
    "     \"Follow the path that honor bids thee walk.\",\n",
    "     \"How should I know? Decide thyself.\"),\n",
    "    (\"Give a toast\",\n",
    "     \"To health and joy, and friends both old and new!\",\n",
    "     \"Drink and be done with it.\"),\n",
    "    (\"Say something kind\",\n",
    "     \"Thou art more lovely than the fairest dawn.\",\n",
    "     \"Thou art adequate, I suppose.\"),\n",
    "    (\"Cheer me up\",\n",
    "     \"Smile, for the world is bright and full of wonder.\",\n",
    "     \"Why should I cheer thee? Cheer thyself.\"),\n",
    "]\n",
    "\n",
    "print(f\"Preference dataset: {len(preference_data)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare preference data for DPO\n",
    "#\n",
    "# For each preference pair, we need the FULL tokenized sequences\n",
    "# (prompt + chosen) and (prompt + rejected), plus masks to identify\n",
    "# which tokens are the response (where we compute log probs).\n",
    "\n",
    "def prepare_dpo_example(prompt, response, max_len=block_size):\n",
    "    \"\"\"Tokenize a (prompt, response) pair and create a response mask.\"\"\"\n",
    "    text = f\"<|user|>{prompt}<|end|><|assistant|>{response}<|end|>\"\n",
    "    tokens = encode_sft(text)\n",
    "    if len(tokens) > max_len:\n",
    "        tokens = tokens[:max_len]\n",
    "\n",
    "    # Find where the response starts (after <|assistant|>)\n",
    "    asst_token_id = SPECIAL_TOKENS['<|assistant|>']\n",
    "    asst_pos = None\n",
    "    for i, t in enumerate(tokens):\n",
    "        if t == asst_token_id:\n",
    "            asst_pos = i\n",
    "            break\n",
    "\n",
    "    # Response mask: 1 for response tokens, 0 for prompt tokens\n",
    "    response_mask = [0] * len(tokens)\n",
    "    if asst_pos is not None:\n",
    "        for i in range(asst_pos + 1, len(tokens)):\n",
    "            response_mask[i] = 1\n",
    "\n",
    "    # Pad\n",
    "    pad_len = max_len - len(tokens)\n",
    "    tokens = tokens + [0] * pad_len\n",
    "    response_mask = response_mask + [0] * pad_len\n",
    "\n",
    "    return tokens, response_mask\n",
    "\n",
    "# Build tensors\n",
    "chosen_ids_list, chosen_masks_list = [], []\n",
    "rejected_ids_list, rejected_masks_list = [], []\n",
    "\n",
    "for prompt, chosen, rejected in preference_data:\n",
    "    c_ids, c_mask = prepare_dpo_example(prompt, chosen)\n",
    "    r_ids, r_mask = prepare_dpo_example(prompt, rejected)\n",
    "    chosen_ids_list.append(c_ids)\n",
    "    chosen_masks_list.append(c_mask)\n",
    "    rejected_ids_list.append(r_ids)\n",
    "    rejected_masks_list.append(r_mask)\n",
    "\n",
    "chosen_ids = torch.tensor(chosen_ids_list, dtype=torch.long, device=device)\n",
    "chosen_masks = torch.tensor(chosen_masks_list, dtype=torch.float, device=device)\n",
    "rejected_ids = torch.tensor(rejected_ids_list, dtype=torch.long, device=device)\n",
    "rejected_masks = torch.tensor(rejected_masks_list, dtype=torch.float, device=device)\n",
    "\n",
    "print(f\"Chosen:   {chosen_ids.shape}\")\n",
    "print(f\"Rejected: {rejected_ids.shape}\")\n",
    "\n",
    "# Visualize one example\n",
    "print(f\"\\nExample: '{preference_data[0][0]}'\")\n",
    "print(f\"  Chosen tokens:  {decode_sft(chosen_ids_list[0][:40])}...\")\n",
    "print(f\"  Chosen mask:    {chosen_masks_list[0][:40]}\")\n",
    "print(f\"  (1 = response token, 0 = prompt/padding)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The core of DPO: compute log probabilities of response tokens\n",
    "\n",
    "def get_response_log_probs(model, token_ids, response_mask):\n",
    "    \"\"\"\n",
    "    Compute the sum of log probabilities for response tokens only.\n",
    "\n",
    "    This is log pi(response | prompt) — the model's log-probability\n",
    "    of generating the response given the prompt.\n",
    "\n",
    "    Args:\n",
    "        model: the language model\n",
    "        token_ids: (B, T) full sequence tokens\n",
    "        response_mask: (B, T) binary mask, 1 for response tokens\n",
    "\n",
    "    Returns:\n",
    "        (B,) sum of log probs over response tokens for each example\n",
    "    \"\"\"\n",
    "    logits, _ = model(token_ids)  # (B, T, vocab)\n",
    "\n",
    "    # Shift: logits[t] predicts token[t+1]\n",
    "    shift_logits = logits[:, :-1, :]         # (B, T-1, vocab)\n",
    "    shift_labels = token_ids[:, 1:]           # (B, T-1)\n",
    "    shift_mask = response_mask[:, 1:]         # (B, T-1)\n",
    "\n",
    "    # Log probabilities\n",
    "    log_probs = F.log_softmax(shift_logits, dim=-1)  # (B, T-1, vocab)\n",
    "\n",
    "    # Gather log probs of actual tokens\n",
    "    token_log_probs = log_probs.gather(2, shift_labels.unsqueeze(-1)).squeeze(-1)  # (B, T-1)\n",
    "\n",
    "    # Mask to only response tokens and sum\n",
    "    masked_log_probs = token_log_probs * shift_mask  # (B, T-1)\n",
    "    return masked_log_probs.sum(dim=-1)  # (B,)\n",
    "\n",
    "print(\"get_response_log_probs defined.\")\n",
    "print(\"This is the building block of DPO — everything else is just algebra.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DPO loss function — this is the entire algorithm\n",
    "\n",
    "def dpo_loss(policy_model, ref_model, chosen_ids, chosen_masks, rejected_ids, rejected_masks, beta=0.1):\n",
    "    \"\"\"\n",
    "    Compute the DPO loss.\n",
    "\n",
    "    L = -log(sigmoid(beta * (log_ratio_chosen - log_ratio_rejected)))\n",
    "\n",
    "    where log_ratio = log(pi_theta(y|x)) - log(pi_ref(y|x))\n",
    "    \"\"\"\n",
    "    # Log probs under the current policy\n",
    "    policy_chosen_logps = get_response_log_probs(policy_model, chosen_ids, chosen_masks)\n",
    "    policy_rejected_logps = get_response_log_probs(policy_model, rejected_ids, rejected_masks)\n",
    "\n",
    "    # Log probs under the reference model (frozen, no grad)\n",
    "    with torch.no_grad():\n",
    "        ref_chosen_logps = get_response_log_probs(ref_model, chosen_ids, chosen_masks)\n",
    "        ref_rejected_logps = get_response_log_probs(ref_model, rejected_ids, rejected_masks)\n",
    "\n",
    "    # Log ratios: how much more does the policy like this vs the reference?\n",
    "    chosen_log_ratio = policy_chosen_logps - ref_chosen_logps\n",
    "    rejected_log_ratio = policy_rejected_logps - ref_rejected_logps\n",
    "\n",
    "    # DPO loss: push chosen_log_ratio up and rejected_log_ratio down\n",
    "    logits = beta * (chosen_log_ratio - rejected_log_ratio)\n",
    "    loss = -F.logsigmoid(logits).mean()\n",
    "\n",
    "    # Useful metrics\n",
    "    with torch.no_grad():\n",
    "        chosen_reward = beta * chosen_log_ratio.mean()\n",
    "        rejected_reward = beta * rejected_log_ratio.mean()\n",
    "        accuracy = (chosen_log_ratio > rejected_log_ratio).float().mean()\n",
    "\n",
    "    return loss, chosen_reward.item(), rejected_reward.item(), accuracy.item()\n",
    "\n",
    "print(\"dpo_loss defined.\")\n",
    "print(\"\\nThat's it. The ENTIRE DPO algorithm is ~20 lines of code.\")\n",
    "print(\"Compare with RLHF: reward model + PPO + KL tuning + 4 models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: DPO Training\n",
    "\n",
    "Now we train. The setup is beautifully simple:\n",
    "- **Policy model**: starts as a copy of SFT model (gets updated)\n",
    "- **Reference model**: frozen copy of SFT model (never changes)\n",
    "- **No reward model needed**\n",
    "- **No RL (PPO) needed**\n",
    "\n",
    "Just a standard training loop with the DPO loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: policy model (trainable) + reference model (frozen)\n",
    "dpo_model = GPT(sft_vocab_size).to(device)\n",
    "dpo_model.load_state_dict(sft_model.state_dict())\n",
    "\n",
    "ref_model = GPT(sft_vocab_size).to(device)\n",
    "ref_model.load_state_dict(sft_model.state_dict())\n",
    "ref_model.eval()\n",
    "for p in ref_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(\"DPO setup:\")\n",
    "print(f\"  Policy model: {sum(p.numel() for p in dpo_model.parameters()):,} params (trainable)\")\n",
    "print(f\"  Ref model:    frozen copy of SFT model\")\n",
    "print(f\"  Total models: 2 (vs 4 for RLHF)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPO Training Loop\n",
    "dpo_lr = 1e-5\n",
    "dpo_beta = 0.1  # Temperature — higher = more conservative changes\n",
    "dpo_iters = 500\n",
    "dpo_batch_size = 8\n",
    "\n",
    "dpo_optimizer = torch.optim.AdamW(dpo_model.parameters(), lr=dpo_lr)\n",
    "\n",
    "# Track metrics\n",
    "dpo_losses = []\n",
    "dpo_chosen_rewards = []\n",
    "dpo_rejected_rewards = []\n",
    "dpo_accuracies = []\n",
    "\n",
    "print(f\"DPO training (beta={dpo_beta}, lr={dpo_lr})...\\n\")\n",
    "dpo_model.train()\n",
    "\n",
    "for iter in range(dpo_iters):\n",
    "    # Sample a batch\n",
    "    batch_idx = torch.randint(len(chosen_ids), (dpo_batch_size,))\n",
    "\n",
    "    loss, c_reward, r_reward, acc = dpo_loss(\n",
    "        dpo_model, ref_model,\n",
    "        chosen_ids[batch_idx], chosen_masks[batch_idx],\n",
    "        rejected_ids[batch_idx], rejected_masks[batch_idx],\n",
    "        beta=dpo_beta\n",
    "    )\n",
    "\n",
    "    dpo_optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(dpo_model.parameters(), 1.0)\n",
    "    dpo_optimizer.step()\n",
    "\n",
    "    dpo_losses.append(loss.item())\n",
    "    dpo_chosen_rewards.append(c_reward)\n",
    "    dpo_rejected_rewards.append(r_reward)\n",
    "    dpo_accuracies.append(acc)\n",
    "\n",
    "    if iter % 100 == 0:\n",
    "        print(f\"  step {iter}: loss {loss.item():.4f}, acc {acc:.2f}, \"\n",
    "              f\"chosen_r {c_reward:+.3f}, rejected_r {r_reward:+.3f}\")\n",
    "\n",
    "print(f\"\\nDPO training done.\")\n",
    "print(f\"Final: loss {dpo_losses[-1]:.4f}, accuracy {dpo_accuracies[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot DPO training metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "window = 30\n",
    "\n",
    "def smooth(data, w):\n",
    "    return [sum(data[max(0,i-w):i+1])/min(i+1,w) for i in range(len(data))]\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(dpo_losses, alpha=0.3, color='blue')\n",
    "axes[0].plot(smooth(dpo_losses, window), color='blue', linewidth=2)\n",
    "axes[0].set_title('DPO Loss')\n",
    "axes[0].set_xlabel('Step')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Implicit rewards (chosen vs rejected)\n",
    "axes[1].plot(smooth(dpo_chosen_rewards, window), color='green', linewidth=2, label='chosen')\n",
    "axes[1].plot(smooth(dpo_rejected_rewards, window), color='red', linewidth=2, label='rejected')\n",
    "axes[1].set_title('Implicit Rewards')\n",
    "axes[1].set_xlabel('Step')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[2].plot(dpo_accuracies, alpha=0.3, color='green')\n",
    "axes[2].plot(smooth(dpo_accuracies, window), color='green', linewidth=2)\n",
    "axes[2].axhline(y=0.5, color='red', linestyle='--', label='random')\n",
    "axes[2].set_title('Preference Accuracy')\n",
    "axes[2].set_xlabel('Step')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observation: the implicit reward for chosen responses goes UP\")\n",
    "print(\"while rejected responses go DOWN — the model learns the preference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Results — SFT vs. DPO\n",
    "\n",
    "Let's see if DPO improved the model's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SFT vs DPO\n",
    "test_prompts = [\n",
    "    \"Write a greeting\",\n",
    "    \"Speak of love\",\n",
    "    \"I am sad\",\n",
    "    \"Give advice\",\n",
    "    \"Tell me a story\",\n",
    "    \"Describe the moon\",  # unseen\n",
    "    \"Write a farewell\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SFT MODEL  vs  DPO MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    sft_response = chat(sft_model, prompt, max_tokens=35)\n",
    "    dpo_response = chat(dpo_model, prompt, max_tokens=35)\n",
    "    print(f\"\\nUser: {prompt}\")\n",
    "    print(f\"  SFT: {sft_response}\")\n",
    "    print(f\"  DPO: {dpo_response}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative: compute implicit rewards for the DPO model\n",
    "# Remember: in DPO, the reward is implicitly r(x,y) = beta * log(pi/pi_ref)\n",
    "\n",
    "dpo_model.eval()\n",
    "ref_model.eval()\n",
    "\n",
    "print(\"Implicit reward scores (higher = model prefers this response more):\")\n",
    "print(\"(DPO model should assign higher implicit reward to chosen vs rejected)\\n\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for prompt, chosen, rejected in preference_data[:8]:\n",
    "        c_ids, c_mask = prepare_dpo_example(prompt, chosen)\n",
    "        r_ids, r_mask = prepare_dpo_example(prompt, rejected)\n",
    "\n",
    "        c_ids_t = torch.tensor([c_ids], dtype=torch.long, device=device)\n",
    "        c_mask_t = torch.tensor([c_mask], dtype=torch.float, device=device)\n",
    "        r_ids_t = torch.tensor([r_ids], dtype=torch.long, device=device)\n",
    "        r_mask_t = torch.tensor([r_mask], dtype=torch.float, device=device)\n",
    "\n",
    "        # Implicit reward = beta * (log_pi - log_pi_ref)\n",
    "        policy_c = get_response_log_probs(dpo_model, c_ids_t, c_mask_t)\n",
    "        ref_c = get_response_log_probs(ref_model, c_ids_t, c_mask_t)\n",
    "        policy_r = get_response_log_probs(dpo_model, r_ids_t, r_mask_t)\n",
    "        ref_r = get_response_log_probs(ref_model, r_ids_t, r_mask_t)\n",
    "\n",
    "        chosen_reward = dpo_beta * (policy_c - ref_c).item()\n",
    "        rejected_reward = dpo_beta * (policy_r - ref_r).item()\n",
    "        is_correct = chosen_reward > rejected_reward\n",
    "        correct += is_correct\n",
    "        total += 1\n",
    "\n",
    "        print(f\"  '{prompt}'\")\n",
    "        print(f\"    Chosen:   {chosen_reward:+.3f}  '{chosen[:35]}...'\")\n",
    "        print(f\"    Rejected: {rejected_reward:+.3f}  '{rejected[:35]}...'\")\n",
    "        print(f\"    {'CORRECT' if is_correct else 'WRONG'}\")\n",
    "        print()\n",
    "\n",
    "print(f\"Preference accuracy: {correct}/{total} = {correct/total:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: DPO vs RLHF — Comparison\n",
    "\n",
    "| | RLHF | DPO |\n",
    "|---|---|---|\n",
    "| **Models needed** | 4 (policy, ref, reward, value) | 2 (policy, ref) |\n",
    "| **Training steps** | Reward model + PPO (two phases) | Single phase |\n",
    "| **Stability** | PPO is notoriously finicky | Standard supervised loss |\n",
    "| **Hyperparameters** | KL beta, clip eps, LR, GAE lambda... | Just beta and LR |\n",
    "| **Reward hacking** | Major risk | Less risk (no explicit reward model to hack) |\n",
    "| **Code complexity** | ~200+ lines for PPO | ~20 lines for DPO loss |\n",
    "| **Memory** | 4 models in GPU memory | 2 models |\n",
    "\n",
    "### When to use which?\n",
    "\n",
    "**DPO is preferred when:**\n",
    "- You have preference data available\n",
    "- You want simplicity and stability\n",
    "- You're doing offline optimization (not interactive)\n",
    "\n",
    "**RLHF is preferred when:**\n",
    "- You need online learning (model generates, humans rate in real-time)\n",
    "- You want to reuse the reward model for evaluation\n",
    "- You need to explore diverse responses (PPO enables this)\n",
    "\n",
    "In practice, most teams now use **DPO or its variants** (IPO, KTO, ORPO) because of the simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual: the effect of beta on DPO\n",
    "#\n",
    "# beta controls how conservative the changes are:\n",
    "# - Small beta → aggressive changes, risk of overfitting to preferences\n",
    "# - Large beta → conservative, stays close to SFT\n",
    "#\n",
    "# This is analogous to the KL penalty in RLHF, but it's baked into the loss.\n",
    "\n",
    "print(\"The role of beta in DPO:\")\n",
    "print()\n",
    "print(\"  beta = 0.01 (very low)\")\n",
    "print(\"    → Aggressive preference optimization\")\n",
    "print(\"    → Model may overfit to preference patterns\")\n",
    "print(\"    → Can lose general language quality\")\n",
    "print()\n",
    "print(\"  beta = 0.1 (moderate — what we used)\")\n",
    "print(\"    → Balanced preference learning\")\n",
    "print(\"    → Good trade-off between preferences and general quality\")\n",
    "print()\n",
    "print(\"  beta = 1.0 (high)\")\n",
    "print(\"    → Very conservative changes\")\n",
    "print(\"    → Stays very close to the SFT model\")\n",
    "print(\"    → Preferences are weakly enforced\")\n",
    "print()\n",
    "print(\"In the DPO paper: beta=0.1 for summarization, beta=0.5 for dialogue.\")\n",
    "print(\"The right value depends on your data and how much you trust the preferences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**What DPO does:**\n",
    "- Directly optimizes the language model on preference pairs\n",
    "- No reward model, no RL — just a clever loss function\n",
    "- The language model implicitly learns its own reward function\n",
    "\n",
    "**The math intuition:**\n",
    "- RLHF objective: maximize reward - KL penalty\n",
    "- Solve analytically → the optimal policy has a closed form\n",
    "- Substitute back → DPO loss, which only needs preference pairs\n",
    "- The reward is implicit: `r(x,y) = beta * log(pi/pi_ref)`\n",
    "\n",
    "**The full post-training stack we built:**\n",
    "\n",
    "```\n",
    "Pre-training (notebook 04)     → Language capability (autocomplete)\n",
    "  ↓\n",
    "SFT (notebook 05)              → Instruction-following format\n",
    "  ↓\n",
    "RLHF (notebook 06) or          → Response quality (preference alignment)\n",
    "DPO  (notebook 07)\n",
    "```\n",
    "\n",
    "You now understand the entire modern LLM training pipeline from scratch.\n",
    "\n",
    "### References\n",
    "- [DPO paper](https://arxiv.org/abs/2305.18290) — the original paper\n",
    "- [Cameron Wolfe's DPO deep dive](https://cameronrwolfe.substack.com/p/direct-preference-optimization) — best explanation\n",
    "- [Raschka's DPO notebook](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb)\n",
    "- [HuggingFace: From RLHF to DPO](https://huggingface.co/blog/ariG23498/rlhf-to-dpo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}