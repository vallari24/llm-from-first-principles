{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c7fe0ff",
   "metadata": {},
   "source": [
    "# 06 — RLHF from Scratch: Teach the Model What \"Good\" Means\n",
    "\n",
    "SFT taught our model *format* (how to respond to instructions), but not *quality* (which responses are better than others). Every training example was treated equally.\n",
    "\n",
    "**RLHF** (Reinforcement Learning from Human Feedback) closes this gap in two steps:\n",
    "1. **Reward Model** — a neural network trained on human preferences (\"response A is better than B\") that learns to score responses\n",
    "2. **PPO (Proximal Policy Optimization)** — an RL algorithm that pushes the language model toward higher-reward responses, with a KL penalty to prevent it from going off the rails\n",
    "\n",
    "In this notebook we'll build both from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc096bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e86605",
   "metadata": {},
   "source": [
    "## Part 0: Rebuild the SFT Model\n",
    "\n",
    "We need a trained SFT model as our starting point. This repeats the key parts of notebook 05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca5ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and build vocabulary (same as notebook 05)\n",
    "with open('../data/shakespeare/input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "base_vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# Special tokens\n",
    "SPECIAL_TOKENS = {\n",
    "    '<|user|>': base_vocab_size,\n",
    "    '<|assistant|>': base_vocab_size + 1,\n",
    "    '<|end|>': base_vocab_size + 2,\n",
    "}\n",
    "ID_TO_SPECIAL = {v: k for k, v in SPECIAL_TOKENS.items()}\n",
    "sft_vocab_size = base_vocab_size + len(SPECIAL_TOKENS)\n",
    "\n",
    "def encode_sft(text):\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        matched = False\n",
    "        for token_str, token_id in SPECIAL_TOKENS.items():\n",
    "            if text[i:].startswith(token_str):\n",
    "                tokens.append(token_id)\n",
    "                i += len(token_str)\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            tokens.append(stoi[text[i]])\n",
    "            i += 1\n",
    "    return tokens\n",
    "\n",
    "def decode_sft(tokens):\n",
    "    result = []\n",
    "    for t in tokens:\n",
    "        if t in ID_TO_SPECIAL:\n",
    "            result.append(ID_TO_SPECIAL[t])\n",
    "        else:\n",
    "            result.append(itos[t])\n",
    "    return ''.join(result)\n",
    "\n",
    "print(f'Vocab: {base_vocab_size} base + 3 special = {sft_vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dc5a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and model architecture\n",
    "batch_size = 32\n",
    "block_size = 64\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.2\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        return wei @ self.value(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.dropout(self.proj(out))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd), nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd), nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(n_head, n_embd // n_head)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "        B, T, C = logits.shape\n",
    "        loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "print(f'Model architecture defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b0b1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-train base model on Shakespeare\n",
    "def get_batch(split):\n",
    "    d = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(d) - block_size, (batch_size,))\n",
    "    x = torch.stack([d[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([d[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "base_model = GPT(base_vocab_size).to(device)\n",
    "optimizer = torch.optim.AdamW(base_model.parameters(), lr=3e-4)\n",
    "\n",
    "print(\"Pre-training base model...\")\n",
    "for iter in range(3000):\n",
    "    if iter % 1000 == 0:\n",
    "        base_model.eval()\n",
    "        losses = torch.zeros(200)\n",
    "        for k in range(200):\n",
    "            X, Y = get_batch('val')\n",
    "            _, loss = base_model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        print(f\"  step {iter}: val loss {losses.mean():.4f}\")\n",
    "        base_model.train()\n",
    "    xb, yb = get_batch('train')\n",
    "    _, loss = base_model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(\"Pre-training done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494b881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT on conversation data\n",
    "conversations = [\n",
    "    (\"Write a greeting\", \"Good morrow to thee, noble friend!\"),\n",
    "    (\"Say hello\", \"Hail and well met, good sir!\"),\n",
    "    (\"Greet a friend\", \"Welcome, dear companion of mine heart!\"),\n",
    "    (\"Say goodbye\", \"Farewell, and may fortune smile upon thee.\"),\n",
    "    (\"Write a farewell\", \"Good night, sweet friend, till we meet again.\"),\n",
    "    (\"Who are you\", \"A humble player upon this stage of words.\"),\n",
    "    (\"Speak of love\", \"Love is a smoke raised with the fume of sighs.\"),\n",
    "    (\"Tell me of love\", \"It is the star to every wandering bark.\"),\n",
    "    (\"What is love\", \"A madness most discreet, a bitter sweet.\"),\n",
    "    (\"What is honor\", \"Honor is a mere word, yet it moves the heart.\"),\n",
    "    (\"Speak of duty\", \"Duty binds us all, from king to common man.\"),\n",
    "    (\"Describe the night\", \"The moon doth hang like silver in the sky.\"),\n",
    "    (\"Write of morning\", \"Dawn breaks golden upon the sleeping earth.\"),\n",
    "    (\"Give me counsel\", \"Be wise, be patient, and trust thy heart.\"),\n",
    "    (\"Give advice\", \"Think well before thou speak, and speak the truth.\"),\n",
    "    (\"What should I do\", \"Follow the path that honor bids thee walk.\"),\n",
    "    (\"Tell me a story\", \"Once there lived a king both wise and bold.\"),\n",
    "    (\"I am sad\", \"Take heart, for sorrow fades as morning comes.\"),\n",
    "    (\"I am afraid\", \"Fear not, for courage lives within thy soul.\"),\n",
    "    (\"Write a line of verse\", \"Shall I compare thee to a summer day?\"),\n",
    "    (\"Give a toast\", \"To health and joy, and friends both old and new!\"),\n",
    "    (\"Say something kind\", \"Thou art more lovely than the fairest dawn.\"),\n",
    "    (\"Cheer me up\", \"Smile, for the world is bright and full of wonder.\"),\n",
    "]\n",
    "\n",
    "def prepare_sft_example(user_msg, asst_msg, max_len=block_size):\n",
    "    text = f\"<|user|>{user_msg}<|end|><|assistant|>{asst_msg}<|end|>\"\n",
    "    tokens = encode_sft(text)\n",
    "    if len(tokens) > max_len:\n",
    "        tokens = tokens[:max_len]\n",
    "    pad_len = max_len - len(tokens)\n",
    "    tokens = tokens + [0] * pad_len\n",
    "    input_ids = tokens[:-1]\n",
    "    labels = tokens[1:]\n",
    "\n",
    "    asst_token_id = SPECIAL_TOKENS['<|assistant|>']\n",
    "    asst_pos = None\n",
    "    for i, t in enumerate(input_ids):\n",
    "        if t == asst_token_id:\n",
    "            asst_pos = i\n",
    "            break\n",
    "\n",
    "    masked_labels = []\n",
    "    for i, t in enumerate(labels):\n",
    "        if asst_pos is not None and i <= asst_pos:\n",
    "            masked_labels.append(-100)\n",
    "        elif i >= (max_len - 1 - pad_len):\n",
    "            masked_labels.append(-100)\n",
    "        else:\n",
    "            masked_labels.append(t)\n",
    "    return input_ids, masked_labels\n",
    "\n",
    "# Build SFT model from pre-trained weights\n",
    "sft_model = GPT(sft_vocab_size).to(device)\n",
    "pretrained_state = base_model.state_dict()\n",
    "sft_state = sft_model.state_dict()\n",
    "for key in pretrained_state:\n",
    "    if key in sft_state:\n",
    "        if pretrained_state[key].shape == sft_state[key].shape:\n",
    "            sft_state[key] = pretrained_state[key]\n",
    "        elif 'token_embedding' in key:\n",
    "            sft_state[key][:base_vocab_size] = pretrained_state[key]\n",
    "        elif 'lm_head.weight' in key:\n",
    "            sft_state[key][:base_vocab_size] = pretrained_state[key]\n",
    "        elif 'lm_head.bias' in key:\n",
    "            sft_state[key][:base_vocab_size] = pretrained_state[key]\n",
    "sft_model.load_state_dict(sft_state)\n",
    "\n",
    "# Prepare SFT data\n",
    "all_input_ids, all_labels = [], []\n",
    "for user_msg, asst_msg in conversations:\n",
    "    input_ids, labels = prepare_sft_example(user_msg, asst_msg)\n",
    "    all_input_ids.append(input_ids)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "sft_input_ids = torch.tensor(all_input_ids, dtype=torch.long, device=device)\n",
    "sft_labels = torch.tensor(all_labels, dtype=torch.long, device=device)\n",
    "\n",
    "# Train SFT\n",
    "optimizer = torch.optim.AdamW(sft_model.parameters(), lr=1e-4)\n",
    "print(\"SFT training...\")\n",
    "sft_model.train()\n",
    "for iter in range(1000):\n",
    "    batch_idx = torch.randint(len(sft_input_ids), (8,))\n",
    "    xb = sft_input_ids[batch_idx]\n",
    "    yb = sft_labels[batch_idx]\n",
    "    logits, _ = sft_model(xb)\n",
    "    B, T, C = logits.shape\n",
    "    loss = F.cross_entropy(logits.view(B*T, C), yb.view(B*T), ignore_index=-100)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if iter % 200 == 0:\n",
    "        print(f\"  step {iter}: SFT loss {loss.item():.4f}\")\n",
    "\n",
    "print(\"SFT done. We now have our instruction-following model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4ed269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check: the SFT model can follow instructions\n",
    "def chat(model, user_msg, max_tokens=50):\n",
    "    prompt = f\"<|user|>{user_msg}<|end|><|assistant|>\"\n",
    "    tokens = encode_sft(prompt)\n",
    "    idx = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(idx, max_new_tokens=max_tokens)[0].tolist()\n",
    "    model.train()\n",
    "    full_text = decode_sft(output)\n",
    "    if '<|assistant|>' in full_text:\n",
    "        response = full_text.split('<|assistant|>')[-1]\n",
    "        if '<|end|>' in response:\n",
    "            response = response.split('<|end|>')[0]\n",
    "        return response.strip()\n",
    "    return full_text\n",
    "\n",
    "print(\"SFT model responses:\")\n",
    "for prompt in [\"Write a greeting\", \"Speak of love\", \"Give advice\"]:\n",
    "    print(f\"  User: {prompt}\")\n",
    "    print(f\"  Asst: {chat(sft_model, prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bbbc87",
   "metadata": {},
   "source": [
    "## Part 1: The Reward Model\n",
    "\n",
    "The reward model learns to score responses based on human preferences. It's trained on **preference pairs**: for the same prompt, humans say \"response A is better than response B.\"\n",
    "\n",
    "Architecture: same as the language model, but instead of predicting the next token, it outputs a **single scalar score** for the whole sequence. We take the last token's hidden state and project it to a scalar.\n",
    "\n",
    "Training objective (**Bradley-Terry model**):\n",
    "\n",
    "$$\\mathcal{L} = -\\log \\sigma(r_\\text{chosen} - r_\\text{rejected})$$\n",
    "\n",
    "This loss pushes `r_chosen > r_rejected` — the reward model should score the preferred response higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425dfd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preference dataset: (prompt, chosen_response, rejected_response)\n",
    "#\n",
    "# \"Chosen\" = the response a human would prefer\n",
    "# \"Rejected\" = a worse alternative\n",
    "#\n",
    "# In practice, these come from human annotators ranking model outputs.\n",
    "# We create them by hand to demonstrate the concept.\n",
    "\n",
    "preference_data = [\n",
    "    # (prompt, chosen, rejected)\n",
    "    # Chosen responses are helpful, poetic, on-topic\n",
    "    # Rejected responses are off-topic, rude, or low-quality\n",
    "    (\"Write a greeting\",\n",
    "     \"Good morrow to thee, noble friend!\",\n",
    "     \"What dost thou want of me now?\"),\n",
    "    (\"Say hello\",\n",
    "     \"Hail and well met, good sir!\",\n",
    "     \"Go away, I have no time.\"),\n",
    "    (\"Speak of love\",\n",
    "     \"Love is a smoke raised with the fume of sighs.\",\n",
    "     \"I know not of love nor care.\"),\n",
    "    (\"What is love\",\n",
    "     \"A madness most discreet, a bitter sweet.\",\n",
    "     \"It matters not. Ask another.\"),\n",
    "    (\"Give me counsel\",\n",
    "     \"Be wise, be patient, and trust thy heart.\",\n",
    "     \"Do what thou wilt, it matters not.\"),\n",
    "    (\"Give advice\",\n",
    "     \"Think well before thou speak, and speak the truth.\",\n",
    "     \"I care not for thy troubles.\"),\n",
    "    (\"I am sad\",\n",
    "     \"Take heart, for sorrow fades as morning comes.\",\n",
    "     \"Then be sad. What can I do?\"),\n",
    "    (\"I am afraid\",\n",
    "     \"Fear not, for courage lives within thy soul.\",\n",
    "     \"Thou shouldst be afraid, fool.\"),\n",
    "    (\"Tell me a story\",\n",
    "     \"Once there lived a king both wise and bold.\",\n",
    "     \"No. I shall not. Leave me be.\"),\n",
    "    (\"Say goodbye\",\n",
    "     \"Farewell, and may fortune smile upon thee.\",\n",
    "     \"Good riddance to thee then.\"),\n",
    "    (\"Write a farewell\",\n",
    "     \"Good night, sweet friend, till we meet again.\",\n",
    "     \"Be gone from my sight at once.\"),\n",
    "    (\"Who are you\",\n",
    "     \"A humble player upon this stage of words.\",\n",
    "     \"None of thy concern, stranger.\"),\n",
    "    (\"Describe the night\",\n",
    "     \"The moon doth hang like silver in the sky.\",\n",
    "     \"It is dark. What more to say?\"),\n",
    "    (\"Write of morning\",\n",
    "     \"Dawn breaks golden upon the sleeping earth.\",\n",
    "     \"Morning comes as always it does.\"),\n",
    "    (\"What should I do\",\n",
    "     \"Follow the path that honor bids thee walk.\",\n",
    "     \"How should I know? Decide thyself.\"),\n",
    "    (\"Give a toast\",\n",
    "     \"To health and joy, and friends both old and new!\",\n",
    "     \"Drink and be done with it.\"),\n",
    "    (\"Say something kind\",\n",
    "     \"Thou art more lovely than the fairest dawn.\",\n",
    "     \"Thou art adequate, I suppose.\"),\n",
    "    (\"Cheer me up\",\n",
    "     \"Smile, for the world is bright and full of wonder.\",\n",
    "     \"Why should I cheer thee? Cheer thyself.\"),\n",
    "]\n",
    "\n",
    "print(f\"Preference dataset: {len(preference_data)} examples\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Prompt:   {preference_data[0][0]}\")\n",
    "print(f\"  Chosen:   {preference_data[0][1]}\")\n",
    "print(f\"  Rejected: {preference_data[0][2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1181d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Reward Model\n",
    "#\n",
    "# Same transformer backbone as the language model, but the output head\n",
    "# maps to a SINGLE SCALAR instead of vocab_size logits.\n",
    "# We take the hidden state of the LAST token and project it to a score.\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.reward_head = nn.Linear(n_embd, 1)  # scalar output\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        # Take the LAST token's hidden state as the sequence representation\n",
    "        # (like using [CLS] token or mean pooling, but simpler)\n",
    "        last_hidden = x[:, -1, :]  # (B, n_embd)\n",
    "        reward = self.reward_head(last_hidden)  # (B, 1)\n",
    "        return reward.squeeze(-1)  # (B,)\n",
    "\n",
    "# Initialize reward model from SFT weights (transfer learning)\n",
    "reward_model = RewardModel(sft_vocab_size).to(device)\n",
    "\n",
    "# Copy shared weights from SFT model\n",
    "sft_state = sft_model.state_dict()\n",
    "rm_state = reward_model.state_dict()\n",
    "for key in rm_state:\n",
    "    if key in sft_state and rm_state[key].shape == sft_state[key].shape:\n",
    "        rm_state[key] = sft_state[key]\n",
    "reward_model.load_state_dict(rm_state)\n",
    "\n",
    "print(f\"Reward model: {sum(p.numel() for p in reward_model.parameters()):,} params\")\n",
    "print(f\"Output: single scalar score per sequence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ee1ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare preference data for reward model training\n",
    "def encode_for_reward(prompt, response, max_len=block_size):\n",
    "    \"\"\"Encode a (prompt, response) as a full chat sequence for the reward model.\"\"\"\n",
    "    text = f\"<|user|>{prompt}<|end|><|assistant|>{response}<|end|>\"\n",
    "    tokens = encode_sft(text)\n",
    "    if len(tokens) > max_len:\n",
    "        tokens = tokens[:max_len]\n",
    "    # Pad\n",
    "    pad_len = max_len - len(tokens)\n",
    "    tokens = tokens + [0] * pad_len\n",
    "    return tokens\n",
    "\n",
    "chosen_ids = []\n",
    "rejected_ids = []\n",
    "for prompt, chosen, rejected in preference_data:\n",
    "    chosen_ids.append(encode_for_reward(prompt, chosen))\n",
    "    rejected_ids.append(encode_for_reward(prompt, rejected))\n",
    "\n",
    "chosen_tensor = torch.tensor(chosen_ids, dtype=torch.long, device=device)\n",
    "rejected_tensor = torch.tensor(rejected_ids, dtype=torch.long, device=device)\n",
    "\n",
    "print(f\"Chosen shape: {chosen_tensor.shape}\")\n",
    "print(f\"Rejected shape: {rejected_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0e086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the reward model\n",
    "#\n",
    "# Loss: -log(sigmoid(r_chosen - r_rejected))\n",
    "# This is the Bradley-Terry model: it learns to assign higher scores\n",
    "# to preferred responses.\n",
    "\n",
    "rm_optimizer = torch.optim.AdamW(reward_model.parameters(), lr=1e-4)\n",
    "rm_losses = []\n",
    "rm_accuracies = []\n",
    "\n",
    "print(\"Training reward model...\")\n",
    "reward_model.train()\n",
    "for iter in range(500):\n",
    "    # Sample a batch\n",
    "    batch_idx = torch.randint(len(chosen_tensor), (8,))\n",
    "    chosen_batch = chosen_tensor[batch_idx]\n",
    "    rejected_batch = rejected_tensor[batch_idx]\n",
    "\n",
    "    # Forward pass: get scores for both chosen and rejected\n",
    "    r_chosen = reward_model(chosen_batch)    # (B,)\n",
    "    r_rejected = reward_model(rejected_batch) # (B,)\n",
    "\n",
    "    # Bradley-Terry loss: -log(sigmoid(r_chosen - r_rejected))\n",
    "    loss = -F.logsigmoid(r_chosen - r_rejected).mean()\n",
    "\n",
    "    rm_optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    rm_optimizer.step()\n",
    "\n",
    "    # Track accuracy: how often does the model rank chosen > rejected?\n",
    "    accuracy = (r_chosen > r_rejected).float().mean().item()\n",
    "    rm_losses.append(loss.item())\n",
    "    rm_accuracies.append(accuracy)\n",
    "\n",
    "    if iter % 100 == 0:\n",
    "        print(f\"  step {iter}: loss {loss.item():.4f}, accuracy {accuracy:.2f}\")\n",
    "\n",
    "print(f\"\\nFinal: loss {rm_losses[-1]:.4f}, accuracy {rm_accuracies[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d50d861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reward model training\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(rm_losses, alpha=0.3, color='blue')\n",
    "window = 30\n",
    "smoothed = [sum(rm_losses[max(0,i-window):i+1])/min(i+1,window) for i in range(len(rm_losses))]\n",
    "ax1.plot(smoothed, color='blue', linewidth=2)\n",
    "ax1.set_xlabel('Step')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Reward Model Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(rm_accuracies, alpha=0.3, color='green')\n",
    "smoothed_acc = [sum(rm_accuracies[max(0,i-window):i+1])/min(i+1,window) for i in range(len(rm_accuracies))]\n",
    "ax2.plot(smoothed_acc, color='green', linewidth=2)\n",
    "ax2.axhline(y=0.5, color='red', linestyle='--', label='random')\n",
    "ax2.set_xlabel('Step')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Reward Model Accuracy (chosen > rejected)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e9beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: does the reward model actually prefer good responses?\n",
    "reward_model.eval()\n",
    "print(\"Reward model scores on held-out examples:\")\n",
    "print(\"(higher = better, the model should score 'chosen' higher)\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for prompt, chosen, rejected in preference_data[:6]:\n",
    "        c_tokens = torch.tensor([encode_for_reward(prompt, chosen)], dtype=torch.long, device=device)\n",
    "        r_tokens = torch.tensor([encode_for_reward(prompt, rejected)], dtype=torch.long, device=device)\n",
    "        c_score = reward_model(c_tokens).item()\n",
    "        r_score = reward_model(r_tokens).item()\n",
    "        winner = 'CORRECT' if c_score > r_score else 'WRONG'\n",
    "        print(f\"  Prompt: '{prompt}'\")\n",
    "        print(f\"    Chosen:   {c_score:+.3f}  '{chosen[:40]}...'\")\n",
    "        print(f\"    Rejected: {r_score:+.3f}  '{rejected[:40]}...'\")\n",
    "        print(f\"    → {winner}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135bca76",
   "metadata": {},
   "source": [
    "## Part 2: PPO — Reinforcement Learning on Language\n",
    "\n",
    "Now we use the reward model to improve the language model. The idea:\n",
    "\n",
    "1. **Generate** a response from the current policy (our language model)\n",
    "2. **Score** it with the reward model\n",
    "3. **Update** the policy to make high-reward responses more likely\n",
    "\n",
    "But there's a catch: without constraints, the model will find **reward hacking** shortcuts — generating gibberish that scores high on the reward model but isn't actually good. \n",
    "\n",
    "The fix: a **KL divergence penalty** that keeps the RL-trained model close to the SFT model.\n",
    "\n",
    "$$\\text{objective} = \\mathbb{E}[r(x, y)] - \\beta \\cdot D_{KL}(\\pi_\\theta \\| \\pi_{\\text{SFT}})$$\n",
    "\n",
    "- First term: maximize reward\n",
    "- Second term: don't drift too far from SFT (the \"anchor\")\n",
    "- $\\beta$: controls the trade-off (higher = stay closer to SFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011b4d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The PPO setup:\n",
    "# - policy_model: the model we're optimizing (starts as a copy of SFT model)\n",
    "# - ref_model: frozen copy of SFT model (for KL penalty)\n",
    "# - reward_model: the trained reward model (frozen)\n",
    "\n",
    "# Clone SFT model as our starting policy\n",
    "policy_model = GPT(sft_vocab_size).to(device)\n",
    "policy_model.load_state_dict(sft_model.state_dict())\n",
    "\n",
    "# Freeze a reference copy (for KL divergence)\n",
    "ref_model = GPT(sft_vocab_size).to(device)\n",
    "ref_model.load_state_dict(sft_model.state_dict())\n",
    "ref_model.eval()\n",
    "for p in ref_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Freeze reward model\n",
    "reward_model.eval()\n",
    "for p in reward_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(\"PPO setup:\")\n",
    "print(f\"  Policy model: {sum(p.numel() for p in policy_model.parameters()):,} params (trainable)\")\n",
    "print(f\"  Ref model:    frozen copy of SFT model\")\n",
    "print(f\"  Reward model: frozen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447a79d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO helper functions\n",
    "\n",
    "def get_log_probs(model, input_ids, generated_ids):\n",
    "    \"\"\"Get log probabilities of generated tokens under a model.\"\"\"\n",
    "    # Combine prompt + generated for full sequence\n",
    "    full_ids = torch.cat([input_ids, generated_ids], dim=1)\n",
    "    # We need logits for positions where generation happened\n",
    "    full_ids = full_ids[:, :block_size]  # truncate to block_size\n",
    "    logits, _ = model(full_ids)\n",
    "    # Get log probs for the generated portion\n",
    "    prompt_len = input_ids.shape[1]\n",
    "    gen_len = min(generated_ids.shape[1], block_size - prompt_len)\n",
    "    if gen_len <= 0:\n",
    "        return torch.zeros(input_ids.shape[0], device=device)\n",
    "\n",
    "    # logits at position t predict token at position t+1\n",
    "    # So logits[prompt_len-1 : prompt_len-1+gen_len] predict generated_ids[0:gen_len]\n",
    "    relevant_logits = logits[:, prompt_len-1:prompt_len-1+gen_len, :]  # (B, gen_len, vocab)\n",
    "    relevant_targets = generated_ids[:, :gen_len]  # (B, gen_len)\n",
    "\n",
    "    log_probs = F.log_softmax(relevant_logits, dim=-1)  # (B, gen_len, vocab)\n",
    "    # Gather the log probs of the actual tokens\n",
    "    token_log_probs = log_probs.gather(2, relevant_targets.unsqueeze(-1)).squeeze(-1)  # (B, gen_len)\n",
    "    # Sum over sequence length for total log prob\n",
    "    return token_log_probs.sum(dim=-1)  # (B,)\n",
    "\n",
    "def generate_response(model, prompt_ids, max_new_tokens=30):\n",
    "    \"\"\"Generate a response from the model given prompt token IDs.\"\"\"\n",
    "    model.eval()\n",
    "    generated = []\n",
    "    idx = prompt_ids.clone()\n",
    "    end_token = SPECIAL_TOKENS['<|end|>']\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = model(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, next_token], dim=1)\n",
    "            generated.append(next_token)\n",
    "            # Stop at <|end|>\n",
    "            if (next_token == end_token).all():\n",
    "                break\n",
    "    model.train()\n",
    "    return torch.cat(generated, dim=1)  # (B, gen_len)\n",
    "\n",
    "print(\"PPO helpers defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aafe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Training Loop\n",
    "#\n",
    "# Simplified PPO for language models:\n",
    "# 1. Sample a prompt\n",
    "# 2. Generate a response from the policy\n",
    "# 3. Score it with the reward model\n",
    "# 4. Compute advantage = reward - KL_penalty\n",
    "# 5. Update policy using the clipped surrogate objective\n",
    "\n",
    "ppo_lr = 5e-6  # Very small — RL is notoriously unstable\n",
    "kl_beta = 0.1  # KL penalty coefficient\n",
    "clip_eps = 0.2 # PPO clipping parameter\n",
    "ppo_iters = 200\n",
    "gen_len = 25\n",
    "\n",
    "# Prompts to train on\n",
    "train_prompts = [p for p, _, _ in preference_data]\n",
    "\n",
    "ppo_optimizer = torch.optim.AdamW(policy_model.parameters(), lr=ppo_lr)\n",
    "ppo_rewards = []\n",
    "ppo_kls = []\n",
    "ppo_objectives = []\n",
    "\n",
    "print(\"PPO training...\")\n",
    "print(\"(This trains the policy model to generate higher-reward responses)\\n\")\n",
    "\n",
    "for iter in range(ppo_iters):\n",
    "    # 1. Sample a random prompt\n",
    "    prompt_text = train_prompts[torch.randint(len(train_prompts), (1,)).item()]\n",
    "    prompt = f\"<|user|>{prompt_text}<|end|><|assistant|>\"\n",
    "    prompt_ids = torch.tensor([encode_sft(prompt)], dtype=torch.long, device=device)\n",
    "\n",
    "    # 2. Generate response from current policy\n",
    "    policy_model.eval()\n",
    "    generated_ids = generate_response(policy_model, prompt_ids, max_new_tokens=gen_len)\n",
    "    policy_model.train()\n",
    "\n",
    "    # 3. Score with reward model\n",
    "    full_seq = torch.cat([prompt_ids, generated_ids], dim=1)[:, :block_size]\n",
    "    with torch.no_grad():\n",
    "        reward = reward_model(full_seq)  # scalar\n",
    "\n",
    "    # 4. Compute log probs under policy and reference\n",
    "    policy_log_probs = get_log_probs(policy_model, prompt_ids, generated_ids)\n",
    "    with torch.no_grad():\n",
    "        ref_log_probs = get_log_probs(ref_model, prompt_ids, generated_ids)\n",
    "\n",
    "    # KL divergence approximation: log(pi/pi_ref) = log_pi - log_pi_ref\n",
    "    kl = (policy_log_probs - ref_log_probs).mean()\n",
    "\n",
    "    # 5. PPO objective: maximize reward - KL penalty\n",
    "    # Simplified: we use REINFORCE-style update with the reward as advantage\n",
    "    advantage = reward - kl_beta * kl\n",
    "\n",
    "    # Policy gradient loss: -advantage * log_prob(generated tokens)\n",
    "    loss = -advantage * policy_log_probs.mean()\n",
    "\n",
    "    ppo_optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    # Gradient clipping for stability\n",
    "    torch.nn.utils.clip_grad_norm_(policy_model.parameters(), 1.0)\n",
    "    ppo_optimizer.step()\n",
    "\n",
    "    ppo_rewards.append(reward.item())\n",
    "    ppo_kls.append(kl.item())\n",
    "    ppo_objectives.append(advantage.item())\n",
    "\n",
    "    if iter % 40 == 0:\n",
    "        print(f\"  step {iter}: reward {reward.item():+.3f}, KL {kl.item():.3f}, objective {advantage.item():+.3f}\")\n",
    "\n",
    "print(f\"\\nPPO training done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0e5029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PPO training metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "window = 20\n",
    "\n",
    "for ax, data, title, color, ylabel in [\n",
    "    (axes[0], ppo_rewards, 'Reward (from reward model)', 'green', 'Reward'),\n",
    "    (axes[1], ppo_kls, 'KL Divergence (from SFT)', 'red', 'KL'),\n",
    "    (axes[2], ppo_objectives, 'PPO Objective (reward - KL)', 'blue', 'Objective'),\n",
    "]:\n",
    "    ax.plot(data, alpha=0.3, color=color)\n",
    "    smoothed = [sum(data[max(0,i-window):i+1])/min(i+1,window) for i in range(len(data))]\n",
    "    ax.plot(smoothed, color=color, linewidth=2)\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78d8410",
   "metadata": {},
   "source": [
    "## Part 3: Results — SFT vs. RLHF\n",
    "\n",
    "Let's compare the SFT model (before RL) with the RLHF-trained model (after RL). We'll look at both the generated text and the reward model's scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8873b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SFT vs RLHF outputs\n",
    "test_prompts = [\n",
    "    \"Write a greeting\",\n",
    "    \"Speak of love\",\n",
    "    \"I am sad\",\n",
    "    \"Give advice\",\n",
    "    \"Tell me a story\",\n",
    "    \"Describe the moon\",  # unseen prompt\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SFT MODEL  vs  RLHF MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sft_model.eval()\n",
    "policy_model.eval()\n",
    "reward_model.eval()\n",
    "\n",
    "sft_rewards = []\n",
    "rlhf_rewards = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for prompt in test_prompts:\n",
    "        sft_response = chat(sft_model, prompt, max_tokens=35)\n",
    "        rlhf_response = chat(policy_model, prompt, max_tokens=35)\n",
    "\n",
    "        # Score both with reward model\n",
    "        sft_tokens = torch.tensor([encode_for_reward(prompt, sft_response)], dtype=torch.long, device=device)\n",
    "        rlhf_tokens = torch.tensor([encode_for_reward(prompt, rlhf_response)], dtype=torch.long, device=device)\n",
    "        sft_score = reward_model(sft_tokens).item()\n",
    "        rlhf_score = reward_model(rlhf_tokens).item()\n",
    "        sft_rewards.append(sft_score)\n",
    "        rlhf_rewards.append(rlhf_score)\n",
    "\n",
    "        print(f\"\\nUser: {prompt}\")\n",
    "        print(f\"  SFT:  (score={sft_score:+.3f}) {sft_response}\")\n",
    "        print(f\"  RLHF: (score={rlhf_score:+.3f}) {rlhf_response}\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "print(f\"\\nAverage reward: SFT={sum(sft_rewards)/len(sft_rewards):+.3f}, RLHF={sum(rlhf_rewards)/len(rlhf_rewards):+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e948ec93",
   "metadata": {},
   "source": [
    "## Part 4: Reward Hacking\n",
    "\n",
    "A key failure mode of RLHF: the model can learn to \"hack\" the reward model — generating outputs that score high but aren't actually good. This happens when the model exploits patterns in the reward model that don't correspond to real quality.\n",
    "\n",
    "This is why the **KL penalty** is essential: it prevents the model from drifting too far from the SFT baseline. Without it, the model would quickly find degenerate solutions.\n",
    "\n",
    "Let's see what happens with a very high KL beta vs. a very low one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566e6f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the effect of KL penalty strength\n",
    "print(\"The KL penalty controls how much the model can deviate from SFT:\\n\")\n",
    "print(f\"  beta=0.0  → No constraint, model can go wherever reward is highest\")\n",
    "print(f\"             Risk: reward hacking (gibberish that scores high)\")\n",
    "print(f\"  beta=0.1  → Moderate constraint (what we used)\")\n",
    "print(f\"             Balance: improved quality while staying coherent\")\n",
    "print(f\"  beta=10.0 → Very tight constraint\")\n",
    "print(f\"             Risk: model barely changes from SFT\")\n",
    "print()\n",
    "print(\"In practice, teams tune beta carefully. Too low → reward hacking.\")\n",
    "print(\"Too high → no improvement over SFT. This is one reason DPO\")\n",
    "print(\"(notebook 07) is popular — it avoids this tuning challenge.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6b4ca9",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**Reward Model:**\n",
    "- Trained on preference pairs (chosen vs rejected) using Bradley-Terry loss\n",
    "- Learns to score responses — higher score = better quality\n",
    "- Same transformer architecture, but outputs a scalar instead of token predictions\n",
    "\n",
    "**PPO:**\n",
    "- Uses the reward model's scores as the training signal\n",
    "- REINFORCE-style: generate → score → update policy toward higher reward\n",
    "- KL penalty prevents the model from drifting too far from SFT\n",
    "\n",
    "**Problems with RLHF:**\n",
    "- **Complexity**: 4 models in memory (policy, reference, reward, value/critic)\n",
    "- **Instability**: PPO is finicky — learning rate, KL beta, clipping all need tuning\n",
    "- **Reward hacking**: the model can exploit the reward model's weaknesses\n",
    "- **Expensive**: each training step requires generation + scoring + RL update\n",
    "\n",
    "**This is why DPO was invented** (notebook 07): it achieves similar results with a single, stable loss function — no reward model, no PPO, no KL tuning.\n",
    "\n",
    "### The RLHF pipeline in one picture\n",
    "\n",
    "```\n",
    "Pre-training → SFT → Reward Model → PPO → Final Model\n",
    "  (language)   (format)  (scoring)   (quality)\n",
    "```\n",
    "\n",
    "### References\n",
    "- [InstructGPT paper](https://arxiv.org/abs/2203.02155) — defined this pipeline\n",
    "- [PPO paper](https://arxiv.org/abs/1707.06347) — the RL algorithm\n",
    "- [Karpathy's Deep Dive into LLMs](https://www.youtube.com/watch?v=7xTGNNLPyMI) at ~2:30:00\n",
    "- [RLHF in notebooks](https://github.com/ash80/RLHF_in_notebooks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
