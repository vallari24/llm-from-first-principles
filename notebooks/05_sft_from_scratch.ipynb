{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 — SFT from Scratch: Turn a Base Model into a Chatbot\n",
    "\n",
    "A pre-trained language model is just an **autocomplete engine**. It predicts the next token based on what came before. Ask it a question and it'll just... continue the text as if it's Shakespeare.\n",
    "\n",
    "**Supervised Fine-Tuning (SFT)** teaches the model a new behavior: given an instruction, produce a helpful response. It does this by training on (instruction, response) pairs with a special twist — we only compute loss on the **assistant's response**, not the user's instruction.\n",
    "\n",
    "In this notebook we'll:\n",
    "1. Pre-train our Shakespeare GPT (base model = autocomplete)\n",
    "2. Add **chat template tokens** (`<|user|>`, `<|assistant|>`, `<|end|>`)\n",
    "3. Create a small conversation dataset\n",
    "4. Implement SFT with **loss masking** — the key ingredient\n",
    "5. See the model shift from autocomplete → instruction-following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pre-train the Base Model\n",
    "\n",
    "Same architecture from notebook 04 — a decoder-only transformer trained on Shakespeare. This gives us a model that can generate Shakespeare-like text, but has no concept of \"instructions\" or \"responses\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Shakespeare data and build character vocabulary\n",
    "with open('../data/shakespeare/input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "base_vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(f'Vocab: {base_vocab_size} chars | Train: {len(train_data):,} | Val: {len(val_data):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "block_size = 64\n",
    "max_iters = 3000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.2\n",
    "\n",
    "def get_batch(split):\n",
    "    d = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(d) - block_size, (batch_size,))\n",
    "    x = torch.stack([d[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([d[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture (same as notebook 04)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        return wei @ v\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.dropout(self.proj(out))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "        B, T, C = logits.shape\n",
    "        loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "model = GPT(base_vocab_size).to(device)\n",
    "print(f'Parameters: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-train on Shakespeare\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train {losses['train']:.4f}, val {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "losses = estimate_loss(model)\n",
    "print(f\"\\nFinal: train {losses['train']:.4f}, val {losses['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base model is just autocomplete — it continues text\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(\"=== Base model: just autocomplete ===\")\n",
    "print(decode(model.generate(context, max_new_tokens=200)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens if we try to \"ask it a question\"?\n",
    "# We encode a question as Shakespeare characters and let it continue\n",
    "question = \"What is love?\"\n",
    "question_tokens = torch.tensor([encode(question)], dtype=torch.long, device=device)\n",
    "print(f\"Prompt: '{question}'\")\n",
    "print(f\"Model continues with:\")\n",
    "output = model.generate(question_tokens, max_new_tokens=150)[0].tolist()\n",
    "print(decode(output))\n",
    "print()\n",
    "print(\"^ It doesn't ANSWER the question — it just continues the text.\")\n",
    "print(\"  It has no concept of 'user asks, assistant responds'.\")\n",
    "print(\"  That's what SFT will teach it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: What is SFT?\n",
    "\n",
    "The base model learned **language** (which characters follow which) but not **behavior** (how to respond to instructions).\n",
    "\n",
    "SFT teaches behavior by fine-tuning on structured conversations:\n",
    "\n",
    "```\n",
    "<|user|>Write a greeting<|end|><|assistant|>Good morrow, friend!<|end|>\n",
    "```\n",
    "\n",
    "Three key ingredients make this work:\n",
    "\n",
    "### 1. Chat Template Tokens\n",
    "Special tokens (`<|user|>`, `<|assistant|>`, `<|end|>`) that the model has **never seen during pre-training**. They create clear boundaries between roles. This is exactly how ChatGPT, Llama, etc. work — each model family has its own chat template.\n",
    "\n",
    "### 2. Loss Masking\n",
    "The critical insight: **we only compute loss on the assistant's response tokens**, not on the user's instruction. Why?\n",
    "- The model shouldn't learn to *generate* user messages — it should learn to *respond* to them\n",
    "- The user tokens provide context, but the gradient only flows through the assistant's words\n",
    "- This is like a student reading a question (no grade) then writing an answer (graded)\n",
    "\n",
    "### 3. The Format Itself\n",
    "By seeing hundreds of (instruction → response) examples, the model learns: when you see `<|assistant|>`, switch from reading to responding. The special tokens become triggers for a new behavior mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Add special tokens to vocabulary\n",
    "#\n",
    "# Real tokenizers (tiktoken, sentencepiece) handle this with special token\n",
    "# registries. We'll do the same thing manually for our character-level model.\n",
    "#\n",
    "# New token IDs are added AFTER the existing vocabulary:\n",
    "#   0-64:  original Shakespeare characters\n",
    "#   65:    <|user|>\n",
    "#   66:    <|assistant|>\n",
    "#   67:    <|end|>\n",
    "\n",
    "SPECIAL_TOKENS = {\n",
    "    '<|user|>': base_vocab_size,          # 65\n",
    "    '<|assistant|>': base_vocab_size + 1,  # 66\n",
    "    '<|end|>': base_vocab_size + 2,        # 67\n",
    "}\n",
    "ID_TO_SPECIAL = {v: k for k, v in SPECIAL_TOKENS.items()}\n",
    "sft_vocab_size = base_vocab_size + len(SPECIAL_TOKENS)  # 68\n",
    "\n",
    "def encode_sft(text):\n",
    "    \"\"\"Encode text, recognizing special tokens as single token IDs.\"\"\"\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        matched = False\n",
    "        for token_str, token_id in SPECIAL_TOKENS.items():\n",
    "            if text[i:].startswith(token_str):\n",
    "                tokens.append(token_id)\n",
    "                i += len(token_str)\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            tokens.append(stoi[text[i]])\n",
    "            i += 1\n",
    "    return tokens\n",
    "\n",
    "def decode_sft(tokens):\n",
    "    \"\"\"Decode token IDs back to text, handling special tokens.\"\"\"\n",
    "    result = []\n",
    "    for t in tokens:\n",
    "        if t in ID_TO_SPECIAL:\n",
    "            result.append(ID_TO_SPECIAL[t])\n",
    "        else:\n",
    "            result.append(itos[t])\n",
    "    return ''.join(result)\n",
    "\n",
    "# Verify it works\n",
    "test = \"<|user|>Hello<|end|><|assistant|>Hi there!<|end|>\"\n",
    "encoded = encode_sft(test)\n",
    "print(f\"Original:  {test}\")\n",
    "print(f\"Encoded:   {encoded}\")\n",
    "print(f\"Decoded:   {decode_sft(encoded)}\")\n",
    "print(f\"\\nNotice: <|user|> is a SINGLE token (65), not 8 separate characters.\")\n",
    "print(f\"Vocab expanded: {base_vocab_size} → {sft_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create the conversation dataset\n",
    "#\n",
    "# Each example is a (user instruction, assistant response) pair.\n",
    "# The responses are Shakespeare-flavored since that's what our model knows.\n",
    "# In practice, SFT datasets have thousands of examples — we use ~40 to\n",
    "# demonstrate the concept with our tiny model.\n",
    "\n",
    "conversations = [\n",
    "    # Greetings\n",
    "    (\"Write a greeting\", \"Good morrow to thee, noble friend!\"),\n",
    "    (\"Say hello\", \"Hail and well met, good sir!\"),\n",
    "    (\"Greet a friend\", \"Welcome, dear companion of mine heart!\"),\n",
    "    (\"Give a welcome\", \"Come in, come in, thou art most welcome here!\"),\n",
    "    # Farewells\n",
    "    (\"Say goodbye\", \"Farewell, and may fortune smile upon thee.\"),\n",
    "    (\"Write a farewell\", \"Good night, sweet friend, till we meet again.\"),\n",
    "    (\"Bid farewell\", \"Part we must, but not in sorrow, friend.\"),\n",
    "    # About self\n",
    "    (\"Who are you\", \"A humble player upon this stage of words.\"),\n",
    "    (\"What are you\", \"I am but a voice shaped by verse and wit.\"),\n",
    "    (\"Introduce yourself\", \"They call me a poet of the written word.\"),\n",
    "    # Love\n",
    "    (\"Speak of love\", \"Love is a smoke raised with the fume of sighs.\"),\n",
    "    (\"Tell me of love\", \"It is the star to every wandering bark.\"),\n",
    "    (\"What is love\", \"A madness most discreet, a bitter sweet.\"),\n",
    "    (\"Write about love\", \"Love looks not with the eyes but with the mind.\"),\n",
    "    # Honor and duty\n",
    "    (\"What is honor\", \"Honor is a mere word, yet it moves the heart.\"),\n",
    "    (\"Speak of duty\", \"Duty binds us all, from king to common man.\"),\n",
    "    (\"Tell of courage\", \"Courage is the fire that burns within the breast.\"),\n",
    "    # Nature\n",
    "    (\"Describe the night\", \"The moon doth hang like silver in the sky.\"),\n",
    "    (\"Write of morning\", \"Dawn breaks golden upon the sleeping earth.\"),\n",
    "    (\"Speak of the sea\", \"The vast and restless ocean knows no peace.\"),\n",
    "    (\"Tell of winter\", \"Cold winds do blow across the barren field.\"),\n",
    "    # Advice\n",
    "    (\"Give me counsel\", \"Be wise, be patient, and trust thy heart.\"),\n",
    "    (\"Give advice\", \"Think well before thou speak, and speak the truth.\"),\n",
    "    (\"What should I do\", \"Follow the path that honor bids thee walk.\"),\n",
    "    (\"Help me decide\", \"Let wisdom guide thee where passion cannot.\"),\n",
    "    # Questions about the world\n",
    "    (\"What news\", \"There is much talk of trouble in the court.\"),\n",
    "    (\"Tell me a story\", \"Once there lived a king both wise and bold.\"),\n",
    "    (\"Name a king\", \"King Henry rules with might and grace alike.\"),\n",
    "    (\"Who rules here\", \"The crown sits heavy on the royal head.\"),\n",
    "    # Emotions\n",
    "    (\"I am sad\", \"Take heart, for sorrow fades as morning comes.\"),\n",
    "    (\"I am afraid\", \"Fear not, for courage lives within thy soul.\"),\n",
    "    (\"I feel lost\", \"Even the wanderer finds his way at last.\"),\n",
    "    (\"I am angry\", \"Let not thy rage consume thy better reason.\"),\n",
    "    # Requests\n",
    "    (\"Write a line of verse\", \"Shall I compare thee to a summer day?\"),\n",
    "    (\"Compose a short poem\", \"The rose doth bloom where gentle waters flow.\"),\n",
    "    (\"Give a toast\", \"To health and joy, and friends both old and new!\"),\n",
    "    (\"Write something wise\", \"The fool doth think he is wise, but nay.\"),\n",
    "    (\"Say something kind\", \"Thou art more lovely than the fairest dawn.\"),\n",
    "    (\"Make me laugh\", \"A fool with wit is wiser than a sage with none!\"),\n",
    "    (\"Cheer me up\", \"Smile, for the world is bright and full of wonder.\"),\n",
    "]\n",
    "\n",
    "print(f\"Created {len(conversations)} conversation pairs\")\n",
    "print(f\"\\nExample formatted conversation:\")\n",
    "example = f\"<|user|>{conversations[0][0]}<|end|><|assistant|>{conversations[0][1]}<|end|>\"\n",
    "print(f\"  {example}\")\n",
    "print(f\"  → {len(encode_sft(example))} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Loss Masking — The Key Insight\n",
    "\n",
    "Here's what makes SFT different from pre-training:\n",
    "\n",
    "```\n",
    "Tokens:  <|user|>  W  r  i  t  e  ...  <|end|>  <|assistant|>  G  o  o  d  ...  <|end|>\n",
    "Loss:      ✗      ✗  ✗  ✗  ✗  ✗  ...    ✗          ✗           ✓  ✓  ✓  ✓  ...   ✓\n",
    "```\n",
    "\n",
    "- **✗ = masked** (loss is 0, no gradient) — everything up to and including `<|assistant|>`\n",
    "- **✓ = computed** (loss contributes to gradient) — the assistant's actual response + final `<|end|>`\n",
    "\n",
    "In code, we use `ignore_index=-100` in `F.cross_entropy` — any target label set to -100 is ignored in the loss computation. This is the standard PyTorch convention used everywhere from HuggingFace to custom training loops.\n",
    "\n",
    "**Why mask?** We want the model to learn *how to respond*, not *how to write user messages*. The user tokens provide context (the model reads them during the forward pass), but we only grade the model on its responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Prepare SFT training data with loss masks\n",
    "#\n",
    "# For each conversation, we create:\n",
    "#   input_ids: the full token sequence (for the forward pass)\n",
    "#   labels:    same as input_ids but shifted, with -100 for masked positions\n",
    "\n",
    "def prepare_sft_example(user_msg, asst_msg, max_len=block_size):\n",
    "    \"\"\"Convert a (user, assistant) pair into (input_ids, labels) with masking.\"\"\"\n",
    "    text = f\"<|user|>{user_msg}<|end|><|assistant|>{asst_msg}<|end|>\"\n",
    "    tokens = encode_sft(text)\n",
    "\n",
    "    # Truncate if needed\n",
    "    if len(tokens) > max_len:\n",
    "        tokens = tokens[:max_len]\n",
    "\n",
    "    # Pad to max_len\n",
    "    pad_len = max_len - len(tokens)\n",
    "    tokens = tokens + [0] * pad_len  # pad with 0 (will be masked anyway)\n",
    "\n",
    "    # input_ids = tokens[:-1], labels = tokens[1:]\n",
    "    # (standard next-token prediction setup)\n",
    "    input_ids = tokens[:-1]\n",
    "    labels = tokens[1:]\n",
    "\n",
    "    # Find where <|assistant|> token is in input_ids\n",
    "    asst_token_id = SPECIAL_TOKENS['<|assistant|>']\n",
    "    asst_pos = None\n",
    "    for i, t in enumerate(input_ids):\n",
    "        if t == asst_token_id:\n",
    "            asst_pos = i\n",
    "            break\n",
    "\n",
    "    # Mask: set labels to -100 for everything up to and including <|assistant|>\n",
    "    # Also mask padding\n",
    "    masked_labels = []\n",
    "    for i, t in enumerate(labels):\n",
    "        if asst_pos is not None and i <= asst_pos:\n",
    "            masked_labels.append(-100)  # masked — don't compute loss here\n",
    "        elif i >= (max_len - 1 - pad_len):  # padding region\n",
    "            masked_labels.append(-100)\n",
    "        else:\n",
    "            masked_labels.append(t)     # compute loss on assistant response\n",
    "\n",
    "    return input_ids, masked_labels\n",
    "\n",
    "# Prepare all examples\n",
    "all_input_ids = []\n",
    "all_labels = []\n",
    "for user_msg, asst_msg in conversations:\n",
    "    input_ids, labels = prepare_sft_example(user_msg, asst_msg)\n",
    "    all_input_ids.append(input_ids)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "sft_input_ids = torch.tensor(all_input_ids, dtype=torch.long, device=device)\n",
    "sft_labels = torch.tensor(all_labels, dtype=torch.long, device=device)\n",
    "\n",
    "print(f\"SFT dataset: {sft_input_ids.shape[0]} examples, each {sft_input_ids.shape[1]} tokens\")\n",
    "\n",
    "# Visualize masking for one example\n",
    "print(f\"\\n--- Example: '{conversations[0][0]}' → '{conversations[0][1]}' ---\")\n",
    "print(f\"\\nFull sequence:\")\n",
    "print(f\"  {decode_sft(all_input_ids[0])}\")\n",
    "print(f\"\\nLabels (what we train on):\")\n",
    "for i, (inp, lab) in enumerate(zip(all_input_ids[0], all_labels[0])):\n",
    "    if lab == -100:\n",
    "        status = \"  MASKED\"\n",
    "    else:\n",
    "        tok_str = decode_sft([lab])\n",
    "        status = f\"  → predict '{tok_str}'\"\n",
    "    if inp == 0 and lab == -100 and i > 30:  # skip padding\n",
    "        if i == 31:\n",
    "            print(f\"  ... (padding, all masked) ...\")\n",
    "        continue\n",
    "    inp_str = decode_sft([inp])\n",
    "    print(f\"  pos {i:2d}: input='{inp_str}'{status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create the SFT model by expanding the pre-trained model's vocabulary\n",
    "#\n",
    "# Key: we copy ALL pre-trained weights, and only randomly initialize\n",
    "# the embeddings for the 3 new special tokens. This preserves everything\n",
    "# the model learned about Shakespeare while giving it new \"switches\" to learn.\n",
    "\n",
    "sft_model = GPT(sft_vocab_size).to(device)\n",
    "\n",
    "# Copy pre-trained weights into the SFT model\n",
    "pretrained_state = model.state_dict()\n",
    "sft_state = sft_model.state_dict()\n",
    "\n",
    "for key in pretrained_state:\n",
    "    if key in sft_state:\n",
    "        if pretrained_state[key].shape == sft_state[key].shape:\n",
    "            # Same shape — copy directly (all internal layers)\n",
    "            sft_state[key] = pretrained_state[key]\n",
    "        elif 'token_embedding' in key:\n",
    "            # Token embeddings: copy first 65 rows, new tokens get random init\n",
    "            sft_state[key][:base_vocab_size] = pretrained_state[key]\n",
    "        elif 'lm_head.weight' in key:\n",
    "            # Output head: copy first 65 rows\n",
    "            sft_state[key][:base_vocab_size] = pretrained_state[key]\n",
    "        elif 'lm_head.bias' in key:\n",
    "            sft_state[key][:base_vocab_size] = pretrained_state[key]\n",
    "\n",
    "sft_model.load_state_dict(sft_state)\n",
    "\n",
    "print(f\"SFT model created: {sft_vocab_size} vocab (was {base_vocab_size})\")\n",
    "print(f\"Pre-trained weights copied. 3 new token embeddings randomly initialized.\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in sft_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The SFT Training Loop\n",
    "\n",
    "Almost identical to pre-training, with one difference: `ignore_index=-100` in the loss function. This single change means the model only learns from the assistant's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT Training\n",
    "sft_lr = 1e-4  # Lower learning rate than pre-training — we're fine-tuning, not training from scratch\n",
    "sft_iters = 1000\n",
    "sft_batch_size = 8\n",
    "\n",
    "optimizer = torch.optim.AdamW(sft_model.parameters(), lr=sft_lr)\n",
    "sft_losses = []\n",
    "\n",
    "sft_model.train()\n",
    "for iter in range(sft_iters):\n",
    "    # Sample a random batch from our conversation dataset\n",
    "    batch_idx = torch.randint(len(sft_input_ids), (sft_batch_size,))\n",
    "    xb = sft_input_ids[batch_idx]   # (B, T)\n",
    "    yb = sft_labels[batch_idx]       # (B, T) — with -100 for masked positions\n",
    "\n",
    "    logits, _ = sft_model(xb)        # (B, T, vocab_size)\n",
    "    B, T, C = logits.shape\n",
    "\n",
    "    # THE KEY LINE: ignore_index=-100 means masked positions contribute 0 to loss\n",
    "    loss = F.cross_entropy(logits.view(B*T, C), yb.view(B*T), ignore_index=-100)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    sft_losses.append(loss.item())\n",
    "    if iter % 200 == 0:\n",
    "        print(f\"step {iter}: SFT loss {loss.item():.4f}\")\n",
    "\n",
    "print(f\"\\nFinal SFT loss: {sft_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SFT training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(sft_losses, alpha=0.3, color='blue', label='raw')\n",
    "# Smoothed\n",
    "window = 50\n",
    "if len(sft_losses) > window:\n",
    "    smoothed = [sum(sft_losses[max(0,i-window):i+1])/min(i+1, window) for i in range(len(sft_losses))]\n",
    "    plt.plot(smoothed, color='blue', linewidth=2, label='smoothed')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('SFT Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Before vs. After\n",
    "\n",
    "Now let's see if the model learned the chat format. We'll prompt it with `<|user|>...<|end|><|assistant|>` and see if it generates a response instead of random autocomplete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(model, user_msg, max_tokens=60):\n",
    "    \"\"\"Send a message to the SFT model and get a response.\"\"\"\n",
    "    prompt = f\"<|user|>{user_msg}<|end|><|assistant|>\"\n",
    "    tokens = encode_sft(prompt)\n",
    "    idx = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(idx, max_new_tokens=max_tokens)[0].tolist()\n",
    "    model.train()\n",
    "\n",
    "    # Decode and extract just the assistant's response\n",
    "    full_text = decode_sft(output)\n",
    "    # Find the response after <|assistant|>\n",
    "    if '<|assistant|>' in full_text:\n",
    "        response = full_text.split('<|assistant|>')[-1]\n",
    "        # Stop at <|end|> if present\n",
    "        if '<|end|>' in response:\n",
    "            response = response.split('<|end|>')[0]\n",
    "        return response.strip()\n",
    "    return full_text\n",
    "\n",
    "def autocomplete(model, text, max_tokens=60):\n",
    "    \"\"\"Use the base model as autocomplete (no chat format).\"\"\"\n",
    "    tokens = encode(text)\n",
    "    idx = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(idx, max_new_tokens=max_tokens)[0].tolist()\n",
    "    model.train()\n",
    "    return decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare base model vs SFT model on the same prompts\n",
    "test_prompts = [\n",
    "    \"Write a greeting\",\n",
    "    \"Speak of love\",\n",
    "    \"Who are you\",\n",
    "    \"I am sad\",\n",
    "    \"Give advice\",       # seen during training\n",
    "    \"Tell a joke\",       # NOT seen during training — can it generalize?\n",
    "    \"Describe the moon\", # NOT seen — can it follow the format?\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BASE MODEL (autocomplete)  vs  SFT MODEL (instruction-following)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nUser: {prompt}\")\n",
    "    print(f\"  Base:  {autocomplete(model, prompt, max_tokens=40)[:80]}...\")\n",
    "    print(f\"  SFT:   {chat(sft_model, prompt, max_tokens=40)}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Catastrophic Forgetting\n",
    "\n",
    "An important side effect of SFT: the model may **forget** some of what it learned during pre-training. This is called **catastrophic forgetting**.\n",
    "\n",
    "Let's check: can the SFT model still generate Shakespeare-like text when prompted *without* the chat template?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test catastrophic forgetting: generate Shakespeare without chat template\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "\n",
    "print(\"=== Base model (free generation) ===\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    base_output = model.generate(context, max_new_tokens=200)[0].tolist()\n",
    "print(decode(base_output))\n",
    "\n",
    "print(\"\\n=== SFT model (free generation — same prompt) ===\")\n",
    "sft_model.eval()\n",
    "with torch.no_grad():\n",
    "    sft_output = sft_model.generate(context, max_new_tokens=200)[0].tolist()\n",
    "print(decode_sft(sft_output))\n",
    "\n",
    "print(\"\\n^ Notice any differences? The SFT model may produce less coherent Shakespeare\")\n",
    "print(\"  because fine-tuning shifted its weights toward the chat format.\")\n",
    "print(\"  This is catastrophic forgetting in action.\")\n",
    "print(\"  In practice, teams mitigate this with:\")\n",
    "print(\"  - Lower learning rate during SFT\")\n",
    "print(\"  - Mixing in some pre-training data during SFT\")\n",
    "print(\"  - LoRA (only fine-tune a small set of adapter weights)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also measure the loss on validation Shakespeare data\n",
    "# to quantify how much the model forgot\n",
    "\n",
    "@torch.no_grad()\n",
    "def measure_shakespeare_loss(m, vocab_sz):\n",
    "    \"\"\"Measure loss on Shakespeare validation data.\"\"\"\n",
    "    m.eval()\n",
    "    losses = []\n",
    "    for _ in range(100):\n",
    "        ix = torch.randint(len(val_data) - block_size, (16,))\n",
    "        x = torch.stack([val_data[i:i+block_size] for i in ix]).to(device)\n",
    "        y = torch.stack([val_data[i+1:i+block_size+1] for i in ix]).to(device)\n",
    "        logits, _ = m(x)\n",
    "        B, T, C = logits.shape\n",
    "        # Only look at the original vocab (first 65 tokens) for fair comparison\n",
    "        loss = F.cross_entropy(logits.view(B*T, C)[:, :base_vocab_size],\n",
    "                               y.view(B*T))\n",
    "        losses.append(loss.item())\n",
    "    m.train()\n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "base_val_loss = measure_shakespeare_loss(model, base_vocab_size)\n",
    "sft_val_loss = measure_shakespeare_loss(sft_model, sft_vocab_size)\n",
    "\n",
    "print(f\"Shakespeare validation loss:\")\n",
    "print(f\"  Base model: {base_val_loss:.4f}\")\n",
    "print(f\"  SFT model:  {sft_val_loss:.4f}\")\n",
    "print(f\"  Difference: {sft_val_loss - base_val_loss:+.4f}\")\n",
    "if sft_val_loss > base_val_loss:\n",
    "    print(f\"\\n  The SFT model is worse at Shakespeare — it traded some language\")\n",
    "    print(f\"  modeling ability for instruction-following ability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**What SFT does:**\n",
    "- Takes a base model (autocomplete) and teaches it a new behavior (instruction-following)\n",
    "- Uses special tokens to define the conversation format\n",
    "- Loss masking ensures the model only learns from its own responses\n",
    "\n",
    "**What SFT does NOT do:**\n",
    "- It doesn't teach the model what a *good* response is vs. a *bad* one\n",
    "- It just teaches the format: \"when you see `<|assistant|>`, generate a response\"\n",
    "- All training examples are treated equally — the model can't learn preferences\n",
    "\n",
    "**This is the gap that RLHF and DPO fill.** SFT teaches *format*, but preference learning teaches *quality*. That's notebook 06 and 07.\n",
    "\n",
    "---\n",
    "\n",
    "### The real-world SFT pipeline\n",
    "\n",
    "What we built here is the core idea. At scale:\n",
    "- Models use subword tokenizers (BPE) instead of characters\n",
    "- SFT datasets have 10K-100K+ examples (we used 40)\n",
    "- LoRA fine-tunes only ~1% of parameters to reduce forgetting\n",
    "- Chat templates vary by model family (Llama uses `[INST]`, ChatGPT uses `<|im_start|>`)\n",
    "- Data quality matters more than quantity — a few thousand expert examples beats millions of noisy ones\n",
    "\n",
    "### References\n",
    "- [InstructGPT paper](https://arxiv.org/abs/2203.02155) — defined the SFT → RLHF pipeline\n",
    "- [SFT from scratch in ~500 lines](https://liyuan24.github.io/writings/supervised_fine_tuning.html)\n",
    "- [Karpathy's Deep Dive into LLMs](https://www.youtube.com/watch?v=7xTGNNLPyMI) at ~1:30:00"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}