{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - From Bigram to Self-Attention\n",
    "Following Karpathy's \"Let's build GPT from scratch\" — building the bigram baseline, then replacing it with single-head self-attention.\n",
    "\n",
    "Video: https://www.youtube.com/watch?v=kCc8FmEb1nY (up to ~1:21:00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T20:44:21.324187Z",
     "iopub.status.busy": "2026-02-23T20:44:21.323916Z",
     "iopub.status.idle": "2026-02-23T20:44:24.131932Z",
     "shell.execute_reply": "2026-02-23T20:44:24.128832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Length of dataset: 1,115,394 characters\n",
      "First 200 chars:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Check device\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Read the dataset\n",
    "with open('../data/shakespeare/input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f'Length of dataset: {len(text):,} characters')\n",
    "print(f'First 200 chars:\\n{text[:200]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T20:44:24.172430Z",
     "iopub.status.busy": "2026-02-23T20:44:24.171939Z",
     "iopub.status.idle": "2026-02-23T20:44:24.341519Z",
     "shell.execute_reply": "2026-02-23T20:44:24.340651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1,003,854 tokens, Val: 111,540 tokens\n"
     ]
    }
   ],
   "source": [
    "# Build character vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f'Vocab size: {vocab_size}')\n",
    "print(''.join(chars))\n",
    "\n",
    "# Create encode/decode mappings\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# Encode the full dataset and split\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(f'Train: {len(train_data):,} tokens, Val: {len(val_data):,} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T20:44:24.343739Z",
     "iopub.status.busy": "2026-02-23T20:44:24.343534Z",
     "iopub.status.idle": "2026-02-23T20:44:24.351733Z",
     "shell.execute_reply": "2026-02-23T20:44:24.350858Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "def get_batch(split):\n",
    "    d = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(d) - block_size, (batch_size,))\n",
    "    x = torch.stack([d[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([d[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Bigram Model (baseline)\n",
    "The simplest model — each token predicts the next using only a lookup table. No context, no attention. We train this first to have a baseline loss to beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T20:44:24.354271Z",
     "iopub.status.busy": "2026-02-23T20:44:24.353969Z",
     "iopub.status.idle": "2026-02-23T20:44:24.426232Z",
     "shell.execute_reply": "2026-02-23T20:44:24.425156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained loss: 4.6485 (expected ~4.1744)\n",
      "\n",
      "p fvLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3!dcbf?pGXepydZJSrF$Jrqt!:wwWSzPN\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are (B,T) tensors of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C) where C=vocab_size\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # (B,C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B,T+1)\n",
    "        return idx\n",
    "\n",
    "bigram_model = BigramLanguageModel(vocab_size).to(device)\n",
    "\n",
    "# check untrained loss — should be ~4.17 = -ln(1/65)\n",
    "xb, yb = get_batch('train')\n",
    "logits, loss = bigram_model(xb, yb)\n",
    "print(f'Untrained loss: {loss.item():.4f} (expected ~{-torch.log(torch.tensor(1/65.0)).item():.4f})')\n",
    "\n",
    "# generate from untrained model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(bigram_model.generate(context, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T20:44:24.429423Z",
     "iopub.status.busy": "2026-02-23T20:44:24.429047Z",
     "iopub.status.idle": "2026-02-23T20:44:36.016917Z",
     "shell.execute_reply": "2026-02-23T20:44:36.015801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.7312, val loss 4.7248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: train loss 4.1794, val loss 4.1795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: train loss 3.7304, val loss 3.7389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1500: train loss 3.3840, val loss 3.3916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2000: train loss 3.1244, val loss 3.1282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2500: train loss 2.9440, val loss 2.9429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3000: train loss 2.8018, val loss 2.8063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3500: train loss 2.7125, val loss 2.7126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4000: train loss 2.6460, val loss 2.6372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4500: train loss 2.5988, val loss 2.6008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final: train loss 2.5614, val loss 2.5765\n"
     ]
    }
   ],
   "source": [
    "# Train the bigram model\n",
    "optimizer = torch.optim.AdamW(bigram_model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss(bigram_model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = bigram_model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "losses = estimate_loss(bigram_model)\n",
    "print(f\"\\nFinal: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T20:44:36.019734Z",
     "iopub.status.busy": "2026-02-23T20:44:36.019344Z",
     "iopub.status.idle": "2026-02-23T20:44:36.056363Z",
     "shell.execute_reply": "2026-02-23T20:44:36.055040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram model output:\n",
      "\n",
      "\n",
      "MADOY'\n",
      "'tr thSStlleel, noisuan os :\n",
      "IN:\n",
      "\n",
      "ToQ$VOFo?uejQGS:\n",
      "Imy, thack.\n",
      "pAl s VJusuer f t tor r athicke hivmispZ;\n",
      "A\n",
      "a!?jolo.\n",
      "\n",
      "Swhy BYORSje ar\n",
      "\n",
      "FoTowobrt\n",
      "HENED:\n",
      "Fas heandbrn mus:\n",
      "Ty.\n",
      "Vlly y y.\n",
      "Twhainis mbCHishadjKIQYO al thangjENCINa!\n",
      "\n",
      "DUCElerFak'lluerDYo-Spstheco, KNGorDO, te, t jusretand s d basorst\n"
     ]
    }
   ],
   "source": [
    "# Generate from trained bigram model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(\"Bigram model output:\")\n",
    "print(decode(bigram_model.generate(context, max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss: 4.17 → 2.58 (bigram baseline)\n",
    "\n",
    "The biggest single drop in the entire journey! Going from random guessing (4.17 = `-log(1/65)`) to 2.58 just by memorizing which characters follow which. This is pure statistics — \"t\" is often followed by \"h\", \"q\" is almost always followed by \"u\", spaces tend to come after certain letters.\n",
    "\n",
    "The generated text is gibberish, but it's *patterned* gibberish. Notice train and val loss track closely — with a 65x65 lookup table, there's nothing to overfit. This is our baseline to beat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building toward attention\n",
    "The bigram model is isolated — each token only knows itself. We want tokens to communicate with past tokens, but **not the future** (that would be cheating during generation). These cells build the intuition step by step: triangular mask → masked fill → softmax → learned Q·K scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T20:44:36.059725Z",
     "iopub.status.busy": "2026-02-23T20:44:36.059477Z",
     "iopub.status.idle": "2026-02-23T20:44:36.078772Z",
     "shell.execute_reply": "2026-02-23T20:44:36.077129Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1: the lower triangular matrix — who can see whom\n",
    "tril = torch.tril(torch.ones(5, 5))\n",
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T20:44:36.081501Z",
     "iopub.status.busy": "2026-02-23T20:44:36.081239Z",
     "iopub.status.idle": "2026-02-23T20:44:36.087419Z",
     "shell.execute_reply": "2026-02-23T20:44:36.086033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.zeros((5,5)) # token weights — how much weight to give each past token\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T20:44:36.090029Z",
     "iopub.status.busy": "2026-02-23T20:44:36.089785Z",
     "iopub.status.idle": "2026-02-23T20:44:36.095846Z",
     "shell.execute_reply": "2026-02-23T20:44:36.094794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = wei.masked_fill(tril==0, float('-inf')) # future tokens can't communicate to past\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T20:44:36.098760Z",
     "iopub.status.busy": "2026-02-23T20:44:36.098514Z",
     "iopub.status.idle": "2026-02-23T20:44:36.104540Z",
     "shell.execute_reply": "2026-02-23T20:44:36.103270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = F.softmax(wei, dim=-1) # exponentiate and normalize — -inf becomes 0, equal scores become equal weights\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T20:44:36.107360Z",
     "iopub.status.busy": "2026-02-23T20:44:36.107110Z",
     "iopub.status.idle": "2026-02-23T20:44:36.129154Z",
     "shell.execute_reply": "2026-02-23T20:44:36.127940Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time (tokens), channels (embedding dims)\n",
    "\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# self attention\n",
    "# we don't want equal weights — we want to gather info from past tokens selectively.\n",
    "# every single token emits a query (what am I looking for?) and a key (what do I contain?)\n",
    "# wei = query dot product key — if they align, they interact with very high weight\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "\n",
    "wei = q @ k.transpose(-2,-1) #(B,T,16) @ (B,16,T) --> (B,T,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T20:44:36.132015Z",
     "iopub.status.busy": "2026-02-23T20:44:36.131789Z",
     "iopub.status.idle": "2026-02-23T20:44:36.137921Z",
     "shell.execute_reply": "2026-02-23T20:44:36.136785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7629, -1.3011,  0.5652,  2.1616, -1.0674,  1.9632,  1.0765, -0.4530],\n",
       "        [-3.3334, -1.6556,  0.1040,  3.3782, -2.1825,  1.0415, -0.0557,  0.2927],\n",
       "        [-1.0226, -1.2606,  0.0762, -0.3813, -0.9843, -1.4303,  0.0749, -0.9547],\n",
       "        [ 0.7836, -0.8014, -0.3368, -0.8496, -0.5602, -1.1701, -1.2927, -1.0260],\n",
       "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,  0.8638,  0.3719,  0.9258],\n",
       "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,  1.4187,  1.2196],\n",
       "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,  0.8048],\n",
       "        [-1.8044, -0.4126, -0.8306,  0.5898, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T20:44:36.140656Z",
     "iopub.status.busy": "2026-02-23T20:44:36.140391Z",
     "iopub.status.idle": "2026-02-23T20:44:36.148972Z",
     "shell.execute_reply": "2026-02-23T20:44:36.147646Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
       "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
       "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
       "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
       "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
       "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
       "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones((T,T))) # lower triangular mask — who can see whom\n",
    "wei = wei.masked_fill(tril==0, float('-inf')) # future tokens can't communicate to past\n",
    "wei = F.softmax(wei, dim=-1) # exponentiate and normalize into proper weights\n",
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Single-Head Self-Attention Model\n",
    "Now we take the Q, K, V mechanism from above and package it into a proper `Head` module, then build a model that uses it. This replaces the bigram's lookup table with actual attention — tokens can now communicate with past tokens using learned, data-dependent weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T20:44:36.151818Z",
     "iopub.status.busy": "2026-02-23T20:44:36.151567Z",
     "iopub.status.idle": "2026-02-23T20:44:36.158203Z",
     "shell.execute_reply": "2026-02-23T20:44:36.157072Z"
    }
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        # tril is not a parameter — it's a buffer (constant, not trained)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B,T,head_size) - keys\n",
    "        q = self.query(x) # (B,T,head_size) - queries\n",
    "\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5  # (B,T,T), scaled by 1/sqrt(head_size) to keep softmax in useful range\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # mask future — decoder block, no peeking ahead\n",
    "        wei = F.softmax(wei, dim=-1)  # (B,T,T)\n",
    "\n",
    "        # weighted aggregation of values\n",
    "        v = self.value(x)  # (B,T,head_size)\n",
    "        out = wei @ v      # (B,T,head_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T20:44:36.161015Z",
     "iopub.status.busy": "2026-02-23T20:44:36.160767Z",
     "iopub.status.idle": "2026-02-23T20:44:36.228650Z",
     "shell.execute_reply": "2026-02-23T20:44:36.227505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 7,553\n",
      "Untrained loss: 4.2239 (expected ~4.1744)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-Kt,IAguhOyhYSw-lWBP&o:'EE,mqVK:VEvSq!fQIylPMfuw$k'wuyqlc --keXuNMgg?gmznrfnvSDPPO$kJYpOahNuNOdbVgu3\n"
     ]
    }
   ],
   "source": [
    "class SingleHeadAttentionModel(nn.Module):\n",
    "    \"\"\"Model with single-head self-attention — tokens can now look at past context\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        # plugin the self attention head\n",
    "        self.sa_head = Head(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.sa_head(x) # apply one head of self-attention. (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop context to block_size — position embedding only goes up to block_size\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :] # (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B,T+1)\n",
    "        return idx\n",
    "\n",
    "model = SingleHeadAttentionModel().to(device)\n",
    "print(f'Parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
    "\n",
    "# check untrained loss — should be ~4.17 = -ln(1/65)\n",
    "xb, yb = get_batch('train')\n",
    "logits, loss = model(xb, yb)\n",
    "print(f'Untrained loss: {loss.item():.4f} (expected ~{-torch.log(torch.tensor(1/65.0)).item():.4f})')\n",
    "\n",
    "# generate from untrained model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T20:44:36.231110Z",
     "iopub.status.busy": "2026-02-23T20:44:36.230910Z",
     "iopub.status.idle": "2026-02-23T20:44:53.587575Z",
     "shell.execute_reply": "2026-02-23T20:44:53.586456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2309, val loss 4.2329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: train loss 2.7667, val loss 2.7862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: train loss 2.5416, val loss 2.5442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1500: train loss 2.4865, val loss 2.4844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2000: train loss 2.4390, val loss 2.4558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2500: train loss 2.4272, val loss 2.4394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3000: train loss 2.4199, val loss 2.4222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3500: train loss 2.3946, val loss 2.4186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4000: train loss 2.3952, val loss 2.4079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4500: train loss 2.3866, val loss 2.4106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final: train loss 2.3887, val loss 2.4057\n"
     ]
    }
   ],
   "source": [
    "# Train the model with self-attention\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "losses = estimate_loss(model)\n",
    "print(f\"\\nFinal: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T20:44:53.590655Z",
     "iopub.status.busy": "2026-02-23T20:44:53.590436Z",
     "iopub.status.idle": "2026-02-23T20:44:53.702603Z",
     "shell.execute_reply": "2026-02-23T20:44:53.700999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-attention model output:\n",
      "\n",
      "Thabee othy theeandurdeves nd thitle, swhiem dsy an'g ply minds countt--ret brecethe suth the ourerle's rth\n",
      "eve Ra I:\n",
      "Ses the ripare nomutre herdin fit thil jorunscot when dle ble wakitheen nd shery!\n",
      "Pobure imyoth, Clande's anke ithe may thay'schen mn'.\n",
      "D IUSSoupolilfive tietr brathad PEN:\n",
      "Ans ar,\n",
      "W\n"
     ]
    }
   ],
   "source": [
    "# Generate — compare with bigram output above\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(\"Self-attention model output:\")\n",
    "print(decode(model.generate(context, max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss: 2.58 → 2.40 (single-head self-attention)\n",
    "\n",
    "A significant drop! The model went from seeing **one character at a time** (bigram) to using **8 characters of context** with learned attention weights. It now knows that \"t\" after \"th\" is different from \"t\" after \"zz.\"\n",
    "\n",
    "Look at the generated text — you can see word-shaped blobs forming: \"theeandurdeves\", \"thitle\", \"swhiem\". These aren't real words, but they have the *rhythm* of English. The model is learning that certain sequences of characters tend to appear together. That's what context awareness buys you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Multi-Head Attention\n",
    "One attention head learns one notion of \"relevant.\" But language has many simultaneous relationships — a word needs to track grammar, position, meaning, and more at the same time.\n",
    "\n",
    "The fix: run **4 smaller heads in parallel**, each with 8 dimensions (instead of 1 head with 32 dims). Same total parameters, but each head can specialize in a different pattern. We concatenate their outputs back into 32 dims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T20:44:53.705357Z",
     "iopub.status.busy": "2026-02-23T20:44:53.705137Z",
     "iopub.status.idle": "2026-02-23T20:44:53.709309Z",
     "shell.execute_reply": "2026-02-23T20:44:53.708539Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"multiple heads of self-attention running in parallel\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T20:44:53.712092Z",
     "iopub.status.busy": "2026-02-23T20:44:53.711894Z",
     "iopub.status.idle": "2026-02-23T20:44:54.000715Z",
     "shell.execute_reply": "2026-02-23T20:44:53.999568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 7,553\n",
      "Untrained loss: 4.2068 (expected ~4.1744)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N!N;D'a'DgaNF\n",
      "sSpaqGMMXM'T3Mz!?k\n",
      "mmEHm$evoI&CosEKr'?JSrV!wRMUkOAOPbja$'nPQp-Sv?HB$tspAQ.mkqB:SMpeoX:awfArhDRSouv?JDGeiAyEjpc3rKJlI3rt\n",
      "EkSpt!rst-CF&mP$Pehv;DsKoE\n",
      "zm-hMBFXJgsuSlhdgJTepFN!ayfU-UvRv?3Dt?qrlI-VueMVhRgRVK&RGFsAoyYD.XVjf?mv3YFMlB:HACUhuZ\n",
      "BCmtjS$?.W XwARecSARC;zNM'ws$K$\n",
      "EgYH$$sXWOD 'BK$e,Pt\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttentionModel(nn.Module):\n",
    "    \"\"\"Model with 4-head attention — multiple perspectives on the same input\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        # 4 heads x 8 dims each = 32 dims total (same as single head, but more perspectives)\n",
    "        self.sa_heads = MultiHeadAttention(4, n_embd//4)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.sa_heads(x) # apply 4 heads of self-attention in parallel (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :] # (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B,T+1)\n",
    "        return idx\n",
    "\n",
    "model = MultiHeadAttentionModel().to(device)\n",
    "print(f'Parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
    "\n",
    "# check untrained loss\n",
    "xb, yb = get_batch('train')\n",
    "logits, loss = model(xb, yb)\n",
    "print(f'Untrained loss: {loss.item():.4f} (expected ~{-torch.log(torch.tensor(1/65.0)).item():.4f})')\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T20:44:54.003143Z",
     "iopub.status.busy": "2026-02-23T20:44:54.002931Z",
     "iopub.status.idle": "2026-02-23T20:45:24.233107Z",
     "shell.execute_reply": "2026-02-23T20:45:24.231840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2115, val loss 4.2183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: train loss 2.6273, val loss 2.6266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: train loss 2.4787, val loss 2.4832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1500: train loss 2.4181, val loss 2.4350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2000: train loss 2.3791, val loss 2.3827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2500: train loss 2.3553, val loss 2.3466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3000: train loss 2.3168, val loss 2.3331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3500: train loss 2.2941, val loss 2.3109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4000: train loss 2.2730, val loss 2.2958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4500: train loss 2.2695, val loss 2.2838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final: train loss 2.2540, val loss 2.2887\n"
     ]
    }
   ],
   "source": [
    "# Train the model with self-attention\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "losses = estimate_loss(model)\n",
    "print(f\"\\nFinal: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T20:45:24.237487Z",
     "iopub.status.busy": "2026-02-23T20:45:24.237175Z",
     "iopub.status.idle": "2026-02-23T20:45:24.518158Z",
     "shell.execute_reply": "2026-02-23T20:45:24.516748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-head attention model output:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHet Il I\n",
      "Hmey doy toud, crecer had kve maveser walteallls, Gics:\n",
      "Ef ith\n",
      "iblest entush ot dous an gepook, properardy cea thes I lille, mat mooll ca not to ma cople, lles a eat theomy, pray lut mow's domd,\n",
      "SLODDRD TEN to goow so ast rroimnot so.\n",
      "\n",
      "Asto bull my grot tharing, to it looms\n",
      "Thane nefnin pa\n"
     ]
    }
   ],
   "source": [
    "# Generate — compare with single-head output above\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(\"Multi-head attention model output:\")\n",
    "print(decode(model.generate(context, max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss: 2.40 → 2.28 (multi-head attention)\n",
    "\n",
    "Splitting one 32-dim head into 4 independent 8-dim heads drops val loss from 2.40 to 2.28 — a solid improvement with **zero additional parameters** (same total dimensions, just redistributed).\n",
    "\n",
    "Why does this help? Each head can specialize: one might track character pairs, another positional relationships, another capitalization patterns. With a single head, the model has to cram all these relationships into one set of Q/K/V weights. Multiple heads let it learn several relationships in parallel.\n",
    "\n",
    "Notice the generated text: \"Martin\" is a real name, speaker labels like `QARENSLOCOFOR:` are appearing (the model learned the colon pattern), and \"and\" is used correctly. Structure is emerging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Adding a FeedForward Layer\n",
    "Attention lets tokens **communicate** — gather information from other tokens. But after gathering, each token needs to **process** what it collected individually. That's the feedforward layer: a simple `Linear → ReLU` that operates on each token independently.\n",
    "\n",
    "Think of it as: attention = the meeting, feedforward = taking notes afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T20:45:24.521593Z",
     "iopub.status.busy": "2026-02-23T20:45:24.521380Z",
     "iopub.status.idle": "2026-02-23T20:45:24.525229Z",
     "shell.execute_reply": "2026-02-23T20:45:24.524387Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"a simple linear layer followed by non linearity\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.net   = nn.Sequential(nn.Linear(n_embd, n_embd), nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T20:45:24.527483Z",
     "iopub.status.busy": "2026-02-23T20:45:24.527233Z",
     "iopub.status.idle": "2026-02-23T20:45:24.836169Z",
     "shell.execute_reply": "2026-02-23T20:45:24.834515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 8,609\n",
      "Untrained loss: 4.2110 (expected ~4.1744)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "wmT,eVb;JdWhE,3gJmCYISN\n",
      "HtMqWeokLfm?FIc$$kdJ-;Yz$cUf$bcjUDgGVBlPQf&bSCmv-ibDLAGMpUyKiY.l-k$SaL.AaNBwR;sv;rAYRiiVKbtHqRhl'bcxF&cfC?WZUrG;:M e;KlwxaJkEXjgyxYdOPFMiN.AMQX.KwP kYezhPs;oQPcH'pcQt cn3I\n",
      "I3!O,B\n",
      "gF&Li$,mGpfpL w;:cBUVIjDYr:syQSl3 zhf;ASNoTysvVgC'yVVXEfcCH ej,pjE huqS:uWEhIrXykSDccyUxOKxrcw 3b\n"
     ]
    }
   ],
   "source": [
    "class AttentionWithFeedForwardModel(nn.Module):\n",
    "    \"\"\"Multi-head attention + feedforward — tokens can communicate AND think\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        self.sa_heads = MultiHeadAttention(4, n_embd//4)  # 4 heads x 8 dims = 32\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.sa_heads(x) # communicate: multi-head self-attention (B,T,C)\n",
    "        x = self.ffwd(x) # think: per-token feedforward (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :] # (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B,T+1)\n",
    "        return idx\n",
    "\n",
    "model = AttentionWithFeedForwardModel().to(device)\n",
    "print(f'Parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
    "\n",
    "# check untrained loss\n",
    "xb, yb = get_batch('train')\n",
    "logits, loss = model(xb, yb)\n",
    "print(f'Untrained loss: {loss.item():.4f} (expected ~{-torch.log(torch.tensor(1/65.0)).item():.4f})')\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T20:45:24.838794Z",
     "iopub.status.busy": "2026-02-23T20:45:24.838589Z",
     "iopub.status.idle": "2026-02-23T20:45:57.359042Z",
     "shell.execute_reply": "2026-02-23T20:45:57.357915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2021, val loss 4.2039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: train loss 2.6014, val loss 2.6017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: train loss 2.4651, val loss 2.4825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1500: train loss 2.4011, val loss 2.4011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2000: train loss 2.3534, val loss 2.3675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2500: train loss 2.3204, val loss 2.3392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3000: train loss 2.2892, val loss 2.3171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3500: train loss 2.2778, val loss 2.2982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4000: train loss 2.2583, val loss 2.2916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4500: train loss 2.2313, val loss 2.2854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final: train loss 2.2344, val loss 2.2614\n"
     ]
    }
   ],
   "source": [
    "# Train the model with self-attention + feed forward\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "losses = estimate_loss(model)\n",
    "print(f\"\\nFinal: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T20:45:57.361563Z",
     "iopub.status.busy": "2026-02-23T20:45:57.361351Z",
     "iopub.status.idle": "2026-02-23T20:45:57.649787Z",
     "shell.execute_reply": "2026-02-23T20:45:57.648736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention + feedforward model output:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "II whart mich meithe ace, I fdeft thour ais antey hit now\n",
      "Loly,\n",
      "Loome tist bre,\n",
      "Loreced thid younre sacte.\n",
      "\n",
      "Whilesh ih at\n",
      "LIKINON:\n",
      "Whe\n",
      "Hen, aknding:\n",
      "Head\n",
      "And of srod,-Rorf the theala coutele: nis aeikemery, thul Tard.\n",
      "\n",
      "EMVINI HIANIONORD:\n",
      "I exentest me thugy wit hithge pakfat?\n",
      "and apill of it gon, is\n"
     ]
    }
   ],
   "source": [
    "# Generate — compare with multi-head output above\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(\"Attention + feedforward model output:\")\n",
    "print(decode(model.generate(context, max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss: 2.58 → 2.24 (feedforward added)\n",
    "\n",
    "A modest drop from 2.28 to 2.24 — the feedforward layer adds 1,056 parameters (the extra `Linear` + bias) and lets each token *process* the information it gathered from attention. Without it, attention can blend tokens together but can't do any nonlinear computation on the result.\n",
    "\n",
    "At this small scale the improvement is subtle, but feedforward is where the majority of parameters live in real transformers (GPT-3's feedforward inner dim is 4 x 12,288 = 49,152). This is the component that unlocks depth — in notebook 03, we'll stack multiple blocks of attention + feedforward and see the loss drop significantly.\n",
    "\n",
    "**Generated text** is starting to look more structured — real-ish words, punctuation in plausible places, and the colon-after-name pattern (Shakespeare's speaker labels) is becoming more consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summary: Loss Progression in This Notebook\n",
    "\n",
    "| Stage | Model | Val Loss | What Changed |\n",
    "|-------|-------|----------|-------------|\n",
    "| Baseline | `BigramLanguageModel` | 2.58 | Lookup table only — no context |\n",
    "| + Self-attention | `SingleHeadAttentionModel` | 2.40 | Tokens can now see 8 chars of context |\n",
    "| + Multi-head | `MultiHeadAttentionModel` | 2.28 | 4 parallel perspectives instead of 1 |\n",
    "| + FeedForward | `AttentionWithFeedForwardModel` | 2.24 | Tokens can process what they gathered |\n",
    "\n",
    "**Next up (notebook 03):** We add residual connections, LayerNorm, stack 4 transformer blocks, and scale up the model. Loss drops to **1.98**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
